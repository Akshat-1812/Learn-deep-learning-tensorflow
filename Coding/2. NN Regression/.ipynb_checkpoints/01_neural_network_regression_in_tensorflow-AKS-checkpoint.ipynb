{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adeb04e",
   "metadata": {},
   "source": [
    "# 01. Neural Network Regression with TensorFlow\n",
    "\n",
    "There are many definitions for a [regression problem](https://en.wikipedia.org/wiki/Regression_analysis) but in our case, we're going to simplify it to be: predicting a number.\n",
    "\n",
    "For example, you might want to:\n",
    "- Predict the selling price of houses given information about them (such as number of rooms, size, number of bathrooms).\n",
    "- Predict the coordinates of a bounding box of an item in an image.\n",
    "- Predict the cost of medical insurance for an individual given their demographics (age, sex, gender, race).\n",
    "\n",
    "In this notebook, we're going to set the foundations for how you can take a sample of inputs (this is your data), build a neural network to discover patterns in those inputs and then make a prediction (in the form of a number) based on those inputs.\n",
    "\n",
    "## What we're going to cover\n",
    "\n",
    "Specifically, we're going to go through doing the following with TensorFlow:\n",
    "- Architecture of a regression model\n",
    "- Input shapes and output shapes\n",
    "  - `X`: features/data (inputs)\n",
    "  - `y`: labels (outputs)\n",
    "- Creating custom data to view and fit\n",
    "- Steps in modelling\n",
    "  - Creating a model\n",
    "  - Compiling a model\n",
    "    - Defining a loss function\n",
    "    - Setting up an optimizer\n",
    "    - Creating evaluation metrics\n",
    "  - Fitting a model (getting it to find patterns in our data)\n",
    "- Evaluating a model\n",
    "  - Visualizng the model (\"visualize, visualize, visualize\")\n",
    "  - Looking at training curves\n",
    "  - Compare predictions to ground truth (using our evaluation metrics)\n",
    "- Saving a model (so we can use it later)\n",
    "- Loading a model\n",
    "\n",
    "Don't worry if none of these make sense now, we're going to go through each.\n",
    "\n",
    "## How you can use this notebook\n",
    "\n",
    "You can read through the descriptions and the code (it should all run), but there's a better option.\n",
    "\n",
    "Write all of the code yourself.\n",
    "\n",
    "Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?\n",
    "\n",
    "You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.\n",
    "\n",
    "Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to **write more code**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26efd3ac",
   "metadata": {},
   "source": [
    "## Typical architecture of a regresison neural network\n",
    "\n",
    "The word *typical* is on purpose.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because there are many different ways (actually, there's almost an infinite number of ways) to write neural networks.\n",
    "\n",
    "But the following is a generic setup for ingesting a collection of numbers, finding patterns in them and then outputing some kind of target number.\n",
    "\n",
    "Yes, the previous sentence is vague but we'll see this in action shortly.\n",
    "\n",
    "| **Hyperparameter** | **Typical value** |\n",
    "| --- | --- |\n",
    "| Input layer shape | Same shape as number of features (e.g. 3 for # bedrooms, # bathrooms, # car spaces in housing price prediction) |\n",
    "| Hidden layer(s) | Problem specific, minimum = 1, maximum = unlimited |\n",
    "| Neurons per hidden layer | Problem specific, generally 10 to 100 |\n",
    "| Output layer shape | Same shape as desired prediction shape (e.g. 1 for house price) |\n",
    "| Hidden activation | Usually [ReLU](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) (rectified linear unit) |\n",
    "| Output activation | None, ReLU, logistic/tanh |\n",
    "| Loss function | [MSE](https://en.wikipedia.org/wiki/Mean_squared_error) (mean square error) or [MAE](https://en.wikipedia.org/wiki/Mean_absolute_error) (mean absolute error)/Huber (combination of MAE/MSE) if outliers |\n",
    "| Optimizer | [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) (stochastic gradient descent), [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) |\n",
    "\n",
    "***Table 1:*** *Typical architecture of a regression network.* ***Source:*** *Adapted from page 293 of [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow Book by Aur√©lien G√©ron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*\n",
    "\n",
    "Again, if you're new to neural networks and deep learning in general, much of the above table won't make sense. But don't worry, we'll be getting hands-on with all of it soon.\n",
    "\n",
    "> üîë **Note:** A **hyperparameter** in machine learning is something a data analyst or developer can set themselves, where as a **parameter** usually describes something a model learns on its own (a value not explicitly set by an analyst).\n",
    "\n",
    "Okay, enough talk, let's get started writing code.\n",
    "\n",
    "To use TensorFlow, we'll import it as the common alias `tf` (short for TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38105cc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07aedff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2cdd71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c92c20",
   "metadata": {},
   "source": [
    "## Creating data to view and fit\n",
    "\n",
    "Since we're working on a **regression problem** (predicting a number) let's create some linear data (a straight line) to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d15f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create features\n",
    "X = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n",
    "\n",
    "# Create labels\n",
    "y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n",
    "\n",
    "# visualize our data\n",
    "plt.scatter(X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc0c2b",
   "metadata": {},
   "source": [
    "Before we do any modelling, can you calculate the pattern between `X` and `y`?\n",
    "\n",
    "For example, say I asked you, based on this data what the `y` value would be if `X` was 17.0?\n",
    "\n",
    "Or how about if `X` was -10.0?\n",
    "\n",
    "This kind of pattern discovery is the essence of what we'll be building neural networks to do for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c352e",
   "metadata": {},
   "source": [
    "## Regression input shapes and output shapes\n",
    "\n",
    "One of the most important concepts when working with neural networks are the input and output shapes.\n",
    "\n",
    "The **input shape** is the shape of your data that goes into the model.\n",
    "\n",
    "The **output shape** is the shape of your data you want to come out of your model.\n",
    "\n",
    "These will differ depending on the problem you're working on.\n",
    "\n",
    "Neural networks accept numbers and output numbers. These numbers are typically represented as tensors (or arrays).\n",
    "\n",
    "Before, we created data using NumPy arrays, but we could do the same with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db4cdbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'bedroom', b'bathroom', b'garage'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([939700])>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example input and output shapes of a regresson model\n",
    "house_info = tf.constant([\"bedroom\", \"bathroom\", \"garage\"])\n",
    "house_price = tf.constant([939700])\n",
    "\n",
    "house_info, house_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "987542fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a92847b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Turn our numpy array into tensors\n",
    "X = tf.cast(tf.constant(X), dtype=tf.float32)\n",
    "y = tf.cast(tf.constant(y), dtype=tf.float32)\n",
    "\n",
    "plt.scatter(X,y);\n",
    "\n",
    "# y = X + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9b711",
   "metadata": {},
   "source": [
    "Our goal here will be to use `X` to predict `y`.\n",
    "\n",
    "So our **input** will be `X` and our **output** will be `y`.\n",
    "\n",
    "Knowing this, what do you think our input and output shapes will be?\n",
    "\n",
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0c1463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([]), TensorShape([]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a single example of X\n",
    "input_shape = X[0].shape \n",
    "\n",
    "# Take a single example of y\n",
    "output_shape = y[0].shape\n",
    "\n",
    "input_shape, output_shape # these are both scalars (no shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40340019",
   "metadata": {},
   "source": [
    "Huh?\n",
    "\n",
    "From this it seems our inputs and outputs have no shape?\n",
    "\n",
    "How could that be?\n",
    "\n",
    "It's because no matter what kind of data we pass to our model, it's always going to take as input and return as ouput some kind of tensor.\n",
    "\n",
    "But in our case because of our dataset (only 2 small lists of numbers), we're looking at a special kind of tensor, more specifically a rank 0 tensor or a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17fe37cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=-7.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the single examples invidually\n",
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb57cf6",
   "metadata": {},
   "source": [
    "In our case, we're trying to build a model to predict the pattern between `X[0]` equalling `-7.0` and `y[0]` equalling `3.0`.\n",
    "\n",
    "So now we get our answer, we're trying to use 1 `X` value to predict 1 `y` value.\n",
    "\n",
    "You might be thinking, \"this seems pretty complicated for just predicting a straight line...\".\n",
    "\n",
    "And you'd be right.\n",
    "\n",
    "But the concepts we're covering here, the concepts of input and output shapes to a model are fundamental. \n",
    "\n",
    "In fact, they're probably two of the things you'll spend the most time on when you work with neural networks: **making sure your input and outputs are in the correct shape**.\n",
    "\n",
    "If it doesn't make sense now, we'll see plenty more examples later on (soon you'll notice the input and output shapes can be almost anything you can imagine).\n",
    "\n",
    "![example of input and output shapes for a housing price prediction problem](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/01-input-and-output-shapes-housing-prices.png)\n",
    "*If you were working on building a machine learning algorithm for predicting housing prices, your inputs may be number of bedrooms, number of bathrooms and number of garages, giving you an input shape of 3 (3 different features). And since you're trying to predict the price of the house, your output shape would be 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1a090",
   "metadata": {},
   "source": [
    "## Steps in modelling with TensorFlow\n",
    "\n",
    "Now we know what data we have as well as the input and output shapes, let's see how we'd build a neural network to model it.\n",
    "\n",
    "In TensorFlow, there are typically 3 fundamental steps to creating and training a model.\n",
    "\n",
    "1. **Creating a model** - piece together the layers of a neural network yourself (using the [Functional](https://www.tensorflow.org/guide/keras/functional) or [Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)) or import a previously built model (known as transfer learning).\n",
    "2. **Compiling a model** - defining how a models performance should be measured (loss/metrics) as well as defining how it should improve (optimizer). \n",
    "3. **Fitting a model** - letting the model try to find patterns in the data (how does `X` get to `y`). \n",
    "\n",
    "Let's see these in action using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) to build a model for our regression data. And then we'll step through each.\n",
    "\n",
    "> **Note:** If you're using [TensorFlow 2.7.0](https://github.com/tensorflow/tensorflow/releases/tag/v2.7.0)+, the `fit()` function no longer upscales input data to go from `(batch_size, )` to `(batch_size, 1)`. To fix this, you'll need to expand the dimension of input data using `tf.expand_dims(input_data, axis=-1)`.\n",
    ">\n",
    "> In our case, this means instead of using `model.fit(X, y, epochs=5)`, use `model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eeb13c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.5048 - mae: 11.5048\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.3723 - mae: 11.3723\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.2398 - mae: 11.2398\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.1073 - mae: 11.1073\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.9748 - mae: 10.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb051b6700>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a model using Sequential API\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "model.compile(loss = tf.keras.losses.mae,\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    "              metrics=['mae'])\n",
    "\n",
    "# Fit our model\n",
    "model.fit(X, y, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba92b2",
   "metadata": {},
   "source": [
    "Boom!\n",
    "\n",
    "We've just trained a model to figure out the patterns between `X` and `y`.\n",
    "\n",
    "How do you think it went?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "327ab1b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out X and y\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b7046",
   "metadata": {},
   "source": [
    "What do you think the outcome should be if we passed our model an `X` value of 17.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30dcc2ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.716021]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a prediction with the model\n",
    "model.predict([17.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f982991",
   "metadata": {},
   "source": [
    "It doesn't go very well... it should've output something close to 27.0.\n",
    "\n",
    "> ü§î **Question:** What's Keras? I thought we were working with TensorFlow but every time we write TensorFlow code, `keras` comes after `tf` (e.g. `tf.keras.layers.Dense()`)?\n",
    "\n",
    "Before TensorFlow 2.0+, [Keras](https://keras.io/) was an API designed to be able to build deep learning models with ease. Since TensorFlow 2.0+, its functionality has been tightly integrated within the TensorFlow library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b67b2",
   "metadata": {},
   "source": [
    "## Improving a model\n",
    "\n",
    "How do you think you'd improve upon our current model?\n",
    "\n",
    "If you guessed by tweaking some of the things we did above, you'd be correct.\n",
    "\n",
    "To improve our model, we alter almost every part of the 3 steps we went through before.\n",
    "\n",
    "1. **Creating a model** - here you might want to add more layers, increase the number of hidden units (also called neurons) within each layer, change the activation functions of each layer.\n",
    "2. **Compiling a model** - you might want to choose optimization function or perhaps change the **learning rate** of the optimization function.\n",
    "3. **Fitting a model** - perhaps you could fit a model for more **epochs** (leave it training for longer) or on more data (give the model more examples to learn from).\n",
    "\n",
    "![various options you can use to improve a neural network model](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-improving-a-model-from-model-perspective.png)\n",
    "*There are many different ways to potentially improve a neural network. Some of the most common include: increasing the number of layers (making the network deeper), increasing the number of hidden units (making the network wider) and changing the learning rate. Because these values are all human-changeable, they're referred to as [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) and the practice of trying to find the best hyperparameters is referred to as [hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization).*\n",
    "\n",
    "Woah. We just introduced a bunch of possible steps. The important thing to remember is how you alter each of these will depend on the problem you're working on.\n",
    "\n",
    "And the good thing is, over the next few problems, we'll get hands-on with all of them.\n",
    "\n",
    "For now, let's keep it simple, all we'll do is train our model for longer (everything else will stay the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ac7859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.2219 - mae: 11.2219\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 11.0894 - mae: 11.0894\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 10.9569 - mae: 10.9569\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.8244 - mae: 10.8244\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.6919 - mae: 10.6919\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.5594 - mae: 10.5594\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 10.4269 - mae: 10.4269\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.2944 - mae: 10.2944\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.1619 - mae: 10.1619\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 10.0294 - mae: 10.0294\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 9.8969 - mae: 9.8969\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7644 - mae: 9.7644\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 9.6319 - mae: 9.6319\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.4994 - mae: 9.4994\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.3669 - mae: 9.3669\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.2344 - mae: 9.2344\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 9.1019 - mae: 9.1019\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.9694 - mae: 8.9694\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.8369 - mae: 8.8369\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7044 - mae: 8.7044\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5719 - mae: 8.5719\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.4394 - mae: 8.4394\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.3069 - mae: 8.3069\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.1744 - mae: 8.1744\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0419 - mae: 8.0419\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.9094 - mae: 7.9094\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.7769 - mae: 7.7769\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.6444 - mae: 7.6444\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5119 - mae: 7.5119\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3794 - mae: 7.3794\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2750 - mae: 7.2750\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.2694 - mae: 7.2694\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.2638 - mae: 7.2638\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2581 - mae: 7.2581\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.2525 - mae: 7.2525\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2469 - mae: 7.2469\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2412 - mae: 7.2412\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.2356 - mae: 7.2356\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2300 - mae: 7.2300\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2244 - mae: 7.2244\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2188 - mae: 7.2188\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2131 - mae: 7.2131\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2075 - mae: 7.2075\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.2019 - mae: 7.2019\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1962 - mae: 7.1962\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1906 - mae: 7.1906\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1850 - mae: 7.1850\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1794 - mae: 7.1794\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1737 - mae: 7.1737\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1681 - mae: 7.1681\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1625 - mae: 7.1625\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1569 - mae: 7.1569\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1512 - mae: 7.1512\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.1456 - mae: 7.1456\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1400 - mae: 7.1400\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1344 - mae: 7.1344\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1287 - mae: 7.1287\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1231 - mae: 7.1231\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1175 - mae: 7.1175\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1119 - mae: 7.1119\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1062 - mae: 7.1062\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1006 - mae: 7.1006\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0950 - mae: 7.0950\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0894 - mae: 7.0894\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 7.0838 - mae: 7.0838\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0781 - mae: 7.0781\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0725 - mae: 7.0725\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0669 - mae: 7.0669\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0613 - mae: 7.0613\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0556 - mae: 7.0556\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0500 - mae: 7.0500\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0444 - mae: 7.0444\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0388 - mae: 7.0388\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0331 - mae: 7.0331\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0275 - mae: 7.0275\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0219 - mae: 7.0219\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0163 - mae: 7.0163\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0106 - mae: 7.0106\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0050 - mae: 7.0050\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.9994 - mae: 6.9994\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9938 - mae: 6.9938\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9881 - mae: 6.9881\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.9825 - mae: 6.9825\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9769 - mae: 6.9769\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9713 - mae: 6.9713\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9656 - mae: 6.9656\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9600 - mae: 6.9600\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9544 - mae: 6.9544\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9488 - mae: 6.9488\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9431 - mae: 6.9431\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9375 - mae: 6.9375\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9319 - mae: 6.9319\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9263 - mae: 6.9263\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9206 - mae: 6.9206\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9150 - mae: 6.9150\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9094 - mae: 6.9094\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9038 - mae: 6.9038\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8981 - mae: 6.8981\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8925 - mae: 6.8925\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.8869 - mae: 6.8869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb06470f40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lecture - 6\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "model.compile(loss = tf.keras.losses.mae,\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    "              metrics = ['mae'])\n",
    "\n",
    "# Fit the model for 100 epoches\n",
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c182cabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.739855]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try and predict what y would be if X was 17.0\n",
    "model.predict([17.0]) # the right answer is 27.0 (y = X + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15057ab",
   "metadata": {},
   "source": [
    "### Best model ‚¨á‚è¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60e3c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.7768 - mae: 13.7768\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.2262 - mae: 13.2262\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12.6900 - mae: 12.6900\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12.1134 - mae: 12.1134\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 11.4349 - mae: 11.4349\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.5682 - mae: 10.5682\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 9.3721 - mae: 9.3721\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5839 - mae: 7.5839\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.6189 - mae: 8.6189\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.0713 - mae: 9.0713\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2100 - mae: 7.2100\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 8.9144 - mae: 8.9144\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.7460 - mae: 8.7460\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7837 - mae: 6.7837\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7147 - mae: 6.7147\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.6417 - mae: 6.6417\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 6.5642 - mae: 6.5642\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.5303 - mae: 6.5303\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.8043 - mae: 7.8043\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7005 - mae: 6.7005\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.4079 - mae: 7.4079\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9019 - mae: 6.9019\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9631 - mae: 6.9631\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4677 - mae: 7.4677\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2305 - mae: 8.2305\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0588 - mae: 6.0588\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.8193 - mae: 8.8193\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5960 - mae: 7.5960\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.8389 - mae: 5.8389\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0189 - mae: 7.0189\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1707 - mae: 6.1707\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.0414 - mae: 8.0414\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8228 - mae: 5.8228\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 7.9824 - mae: 7.9824\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2004 - mae: 7.2004\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3092 - mae: 5.3092\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4407 - mae: 6.4407\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9717 - mae: 5.9717\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.4547 - mae: 7.4547\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8631 - mae: 4.8631\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.8137 - mae: 8.8137\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7748 - mae: 7.7748\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.4332 - mae: 5.4332\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.6172 - mae: 6.6172\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8410 - mae: 7.8410\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 5.5571 - mae: 5.5571\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6628 - mae: 5.6628\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5290 - mae: 6.5290\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7652 - mae: 3.7652\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3186 - mae: 5.3186\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9266 - mae: 4.9266\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.6458 - mae: 7.6458\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.9981 - mae: 4.9981\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7389 - mae: 4.7389\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.4958 - mae: 7.4958\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8073 - mae: 3.8073\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.6994 - mae: 6.6994\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.7296 - mae: 6.7296\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6947 - mae: 2.6947\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 8.7562 - mae: 8.7562\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1185 - mae: 7.1185\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4209 - mae: 3.4209\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9792 - mae: 5.9792\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 7.1573 - mae: 7.1573\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4696 - mae: 3.4696\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0926 - mae: 5.0926\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0807 - mae: 7.0807\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.2619 - mae: 3.2619\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.8396 - mae: 4.8396\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9702 - mae: 6.9702\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8364 - mae: 2.8364\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3946 - mae: 5.3946\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.4462 - mae: 6.4462\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4438 - mae: 2.4438\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4973 - mae: 5.4973\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5082 - mae: 6.5082\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9467 - mae: 1.9467\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1562 - mae: 6.1562\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0599 - mae: 6.0599\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4427 - mae: 1.4427\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7944 - mae: 6.7944\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7333 - mae: 5.7333\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4518 - mae: 1.4518\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4430 - mae: 6.4430\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.4389 - mae: 5.438 - 0s 2ms/step - loss: 5.4389 - mae: 5.4389\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3028 - mae: 1.3028\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4798 - mae: 6.4798\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1000us/step - loss: 5.1781 - mae: 5.1781\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0770 - mae: 1.0770\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6867 - mae: 6.6867\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9480 - mae: 4.9480\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8098 - mae: 0.8098\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9711 - mae: 6.9711\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.7452 - mae: 4.7452\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5410 - mae: 0.5410\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.8123 - mae: 1.8123\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9680 - mae: 5.9680\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0080 - mae: 2.0080\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2283 - mae: 5.2283\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4528 - mae: 4.4528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb0666f9d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(50),\n",
    "    tf.keras.layers.Dense(25),\n",
    "    tf.keras.layers.Dense(8),\n",
    "    tf.keras.layers.Dense(4),\n",
    "    tf.keras.layers.Dense(2),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "model.compile(loss = tf.keras.losses.mae,\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    "              metrics=[\"mae\"])\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80338b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.predict([17.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a36a0e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26.311575]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([17.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd04688d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65.3604]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual -> 66.0\n",
    "model.predict([56.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0092faf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.1407 - mae: 13.1407\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.4751 - mae: 12.4751\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 11.8029 - mae: 11.8029\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.1216 - mae: 11.1216\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.4260 - mae: 10.4260\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7259 - mae: 9.7259\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.9961 - mae: 8.9961\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 8.2333 - mae: 8.2333\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.4276 - mae: 7.4276\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5690 - mae: 6.5690\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6583 - mae: 5.6583\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6803 - mae: 4.6803\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1211 - mae: 4.1211\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0120 - mae: 4.0120\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 4.0106 - mae: 4.0106\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9347 - mae: 3.9347\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9456 - mae: 3.9456\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9462 - mae: 3.9462\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9203 - mae: 3.9203\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9537 - mae: 3.9537\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8948 - mae: 3.8948\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9614 - mae: 3.9614\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8919 - mae: 3.8919\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9462 - mae: 3.9462\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.9064 - mae: 3.9064\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9264 - mae: 3.9264\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9140 - mae: 3.9140\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9008 - mae: 3.9008\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9217 - mae: 3.9217\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8752 - mae: 3.8752\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9295 - mae: 3.9295\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8604 - mae: 3.8604\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9299 - mae: 3.9299\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8752 - mae: 3.8752\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9059 - mae: 3.9059\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8831 - mae: 3.8831\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8800 - mae: 3.8800\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8911 - mae: 3.8911\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8541 - mae: 3.8541\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8991 - mae: 3.8991\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8315 - mae: 3.8315\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.9101 - mae: 3.9101\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8453 - mae: 3.8453\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.8841 - mae: 3.884 - 0s 4ms/step - loss: 3.8841 - mae: 3.8841\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8534 - mae: 3.8534\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8581 - mae: 3.8581\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8616 - mae: 3.8616\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8320 - mae: 3.8320\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8698 - mae: 3.8698\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8099 - mae: 3.8099\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8850 - mae: 3.8850\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8164 - mae: 3.8164\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8613 - mae: 3.8613\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8247 - mae: 3.8247\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.8348 - mae: 3.8348\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8331 - mae: 3.8331\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8084 - mae: 3.8084\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8420 - mae: 3.8420\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7881 - mae: 3.7881\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8569 - mae: 3.8569\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7887 - mae: 3.7887\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8371 - mae: 3.8371\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.7972 - mae: 3.7972\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8103 - mae: 3.8103\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8058 - mae: 3.8058\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7836 - mae: 3.7836\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8177 - mae: 3.8177\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7629 - mae: 3.7629\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 3.8300 - mae: 3.8300\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 3.7620 - mae: 3.7620\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8115 - mae: 3.8115\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7707 - mae: 3.7707\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.7845 - mae: 3.7845\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7796 - mae: 3.7796\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7574 - mae: 3.7574\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7944 - mae: 3.7944\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7364 - mae: 3.7364\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8042 - mae: 3.8042\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7365 - mae: 3.7365\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7846 - mae: 3.7846\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.7454 - mae: 3.7454\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7573 - mae: 3.7573\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7544 - mae: 3.7544\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.7322 - mae: 3.7322\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.7704 - mae: 3.7704\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7085 - mae: 3.7085\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.7794 - mae: 3.7794\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7120 - mae: 3.7120\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.7563 - mae: 3.7563\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7211 - mae: 3.7211\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7287 - mae: 3.7287\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.7304 - mae: 3.7304\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.7058 - mae: 3.7058\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7464 - mae: 3.7464\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6795 - mae: 3.6795\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7544 - mae: 3.7544\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.6886 - mae: 3.6886\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7266 - mae: 3.7266\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.6979 - mae: 3.6979\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.6986 - mae: 3.6986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb06b4ca30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mae\",\n",
    "              optimizer = 'sgd',\n",
    "              metrics=[\"mae\"]\n",
    "              )\n",
    "\n",
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33abd3ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.624386]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([17.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b99f661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.3469 - mae: 12.3469\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.6170 - mae: 11.6170\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.8837 - mae: 10.8837\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.1465 - mae: 10.1465\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4040 - mae: 9.4040\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.6532 - mae: 8.6532\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.8904 - mae: 7.8904\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1120 - mae: 7.1120\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8174 - mae: 6.8174\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1412 - mae: 7.1412\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.4160 - mae: 7.4160\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7229 - mae: 7.7229\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.7818 - mae: 7.7818\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6395 - mae: 7.6395\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3493 - mae: 7.3493\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0024 - mae: 7.0024\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7269 - mae: 6.7269\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.4364 - mae: 6.4364\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.1431 - mae: 6.1431\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0919 - mae: 6.0919\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 6.0366 - mae: 6.0366\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1774 - mae: 6.1774\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.2159 - mae: 6.2159\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1539 - mae: 6.1539\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0036 - mae: 6.0036\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7739 - mae: 5.7739\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.5313 - mae: 5.5313\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4211 - mae: 5.4211\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3081 - mae: 5.3081\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2965 - mae: 5.2965\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2780 - mae: 5.2780\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2141 - mae: 5.2141\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1082 - mae: 5.1082\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9639 - mae: 4.9639\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7844 - mae: 4.7844\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.5777 - mae: 4.5777\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4758 - mae: 4.4758\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3666 - mae: 4.3666\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2498 - mae: 4.2498\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.1323 - mae: 4.1323\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.9836 - mae: 3.9836\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.8347 - mae: 3.8347\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6784 - mae: 3.6784\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5146 - mae: 3.5146\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.3429 - mae: 3.3429\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.1692 - mae: 3.1692\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9985 - mae: 2.9985\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8128 - mae: 2.8128\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6285 - mae: 2.6285\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4338 - mae: 2.4338\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2285 - mae: 2.2285\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0123 - mae: 2.0123\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7852 - mae: 1.7852\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.5469 - mae: 1.5469\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2972 - mae: 1.2972\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0368 - mae: 1.0368\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7763 - mae: 0.7763\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5451 - mae: 0.5451\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2720 - mae: 0.2720\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0768 - mae: 0.0768\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3693 - mae: 0.3693\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4941 - mae: 0.4941\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5882 - mae: 0.5882\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7020 - mae: 0.7020\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6968 - mae: 0.6968\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7073 - mae: 0.7073\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6074 - mae: 0.6074\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5281 - mae: 0.5281\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4097 - mae: 0.4097\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2869 - mae: 0.2869\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2769 - mae: 0.2769\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0608 - mae: 0.0608\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4015 - mae: 0.4015\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4353 - mae: 0.4353\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2603 - mae: 0.2603\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3222 - mae: 0.3222\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2612 - mae: 0.2612\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2212 - mae: 0.2212\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1594 - mae: 0.1594\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0904 - mae: 0.0904\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1716 - mae: 0.1716\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1388 - mae: 0.1388\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2165 - mae: 0.2165\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1966 - mae: 0.1966\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1677 - mae: 0.1677\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1395 - mae: 0.1395\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0454 - mae: 0.0454\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1130 - mae: 0.1130\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1426 - mae: 0.1426\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1916 - mae: 0.1916\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1534 - mae: 0.1534\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1144 - mae: 0.1144\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0577 - mae: 0.0577\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1123 - mae: 0.1123\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.1674 - mae: 0.1674\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1911 - mae: 0.1911\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1582 - mae: 0.1582\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1128 - mae: 0.1128\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1844 - mae: 0.1844\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0933 - mae: 0.0933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb07d89e20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mae\",\n",
    "              optimizer = tf.keras.optimizers.Adam(lr=0.01),\n",
    "              metrics=[\"mae\"]\n",
    "              )\n",
    "\n",
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2930d36c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB0801D280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[26.209171]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([17.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ce03e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.611385]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([56.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ae6f9",
   "metadata": {},
   "source": [
    "## Evaluating a model \n",
    "\n",
    "A typical workflow you'll go through when building neural networks is:\n",
    "\n",
    "```\n",
    "Build a model -> evaluate it -> build (tweak) a model -> evaulate it -> build (tweak) a model -> evaluate it...\n",
    "```\n",
    "\n",
    "The tweaking comes from maybe not building a model from scratch but adjusting an existing one.\n",
    "\n",
    "### Visualize, visualize, visualize\n",
    "\n",
    "When it comes to evaluation, you'll want to remember the words: \"visualize, visualize, visualize.\" \n",
    "\n",
    "This is because you're probably better looking at something (doing) than you are thinking about something.\n",
    "\n",
    "It's a good idea to visualize:\n",
    "* **The data** - what data are you working with? What does it look like?\n",
    "* **The model itself** - what does the architecture look like? What are the different shapes?\n",
    "* **The training of a model** - how does a model perform while it learns?\n",
    "* **The predictions of a model** - how do the predictions of a model line up against the ground truth (the original labels)?\n",
    "\n",
    "Let's start by visualizing the model.\n",
    "\n",
    "But first, we'll create a little bit of a bigger dataset and a new model we can use (it'll be the same as before, but the more practice the better).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c54e3c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100,  -96,  -92,  -88,  -84,  -80,  -76,  -72,  -68,  -64,  -60,\n",
       "        -56,  -52,  -48,  -44,  -40,  -36,  -32,  -28,  -24,  -20,  -16,\n",
       "        -12,   -8,   -4,    0,    4,    8,   12,   16,   20,   24,   28,\n",
       "         32,   36,   40,   44,   48,   52,   56,   60,   64,   68,   72,\n",
       "         76,   80,   84,   88,   92,   96])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a bigger dataset\n",
    "X = np.arange(-100, 100, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e396f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42,\n",
       "       -38, -34, -30, -26, -22, -18, -14, -10,  -6,  -2,   2,   6,  10,\n",
       "        14,  18,  22,  26,  30,  34,  38,  42,  46,  50,  54,  58,  62,\n",
       "        66,  70,  74,  78,  82,  86,  90,  94,  98, 102, 106])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make labels for data set\n",
    "y = X+10\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc5a47fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1eb080ce430>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the data\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2aed36",
   "metadata": {},
   "source": [
    "## Split data into training/test set\n",
    "\n",
    "One of the other most common and important steps in a machine learning project is creating a training and test set (and when required, a validation set).\n",
    "\n",
    "Each set serves a specific purpose:\n",
    "* **Training set** - the model learns from this data, which is typically 70-80% of the total data available (like the course materials you study during the semester).\n",
    "* **Validation set** - the model gets tuned on this data, which is typically 10-15% of the total data available (like the practice exam you take before the final exam).\n",
    "* **Test set** - the model gets evaluated on this data to test what it has learned, it's typically 10-15% of the total data available (like the final exam you take at the end of the semester).\n",
    "\n",
    "For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on.\n",
    "\n",
    "We can create them by splitting our `X` and `y` arrays.\n",
    "\n",
    "> üîë **Note:** When dealing with real-world data, this step is typically done right at the start of a project (the test set should always be kept separate from all other data). We want our model to learn on training data and then evaluate it on test data to get an indication of how well it **generalizes** to unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c72d8a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train = X[:40] # first 40 examples (80% of data)\n",
    "y_train = y[:40]\n",
    "\n",
    "X_test = X[40:] # last 10 examples (20% of data)\n",
    "y_test = y[40:]\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a0e3b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60, 64, 68, 72, 76, 80, 84, 88, 92, 96])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaec20a",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "\n",
    "Now we've got our training and test data, it's a good idea to visualize it.\n",
    "\n",
    "Let's plot it with some nice colours to differentiate what's what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f26cc1e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Training Data\n",
    "plt.scatter(X_train, y_train, c='b', label='Training Data')\n",
    "\n",
    "# Test data\n",
    "plt.scatter(X_test, y_test, c='g', label='Testing Data')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcfa8e",
   "metadata": {},
   "source": [
    "Beautiful! Any time you can visualize your data, your model, your anything, it's a good idea. \n",
    "\n",
    "With this graph in mind, what we'll be trying to do is build a model which learns the pattern in the blue dots (`X_train`) to draw the green dots (`X_test`).\n",
    "\n",
    "Time to build a model. We'll make the exact same one from before (the one we trained for longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dec79d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(50),\n",
    "    tf.keras.layers.Dense(25),\n",
    "    tf.keras.layers.Dense(8),\n",
    "    tf.keras.layers.Dense(4),\n",
    "    tf.keras.layers.Dense(2),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "model.compile(loss = tf.keras.losses.mae,\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    "              metrics=[\"mae\"])\n",
    "\n",
    "# Fit\n",
    "# model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f11506",
   "metadata": {},
   "source": [
    "## Visualizing the model\n",
    "\n",
    "After you've built a model, you might want to take a look at it (especially if you haven't built many before).\n",
    "\n",
    "You can take a look at the layers and shapes of your model by calling [`summary()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary) on it.\n",
    "\n",
    "> üîë **Note:** Visualizing a model is particularly helpful when you run into input and output shape mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e266735b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7d09d31d4e66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Doesn't work (model not fit/built)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   2349\u001b[0m     \"\"\"\n\u001b[0;32m   2350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2351\u001b[1;33m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[0;32m   2352\u001b[0m                        \u001b[1;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2353\u001b[0m                        \u001b[1;34m'`fit()` with some data, or specify '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "# Doesn't work (model not fit/built)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2aa4ca",
   "metadata": {},
   "source": [
    "Ahh, the cell above errors because we haven't fit our built our model.\n",
    "\n",
    "We also haven't told it what input shape it should be expecting.\n",
    "\n",
    "Remember above, how we discussed the input shape was just one number?\n",
    "\n",
    "We can let our model know the input shape of our data using the `input_shape` parameter to the first layer (usually if `input_shape` isn't defined, Keras tries to figure it out automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c6f0308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(10, input_shape=[1]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "   \n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "model.compile(loss = tf.keras.losses.mae,\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    "              metrics=[\"mae\"])\n",
    "\n",
    "# Fit\n",
    "# model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f5c4366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 10)                20        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599a2be",
   "metadata": {},
   "source": [
    "Calling `summary()` on our model shows us the layers it contains, the output shape and the number of parameters.\n",
    "* **Total params** - total number of parameters in the model.\n",
    "* **Trainable parameters** - these are the parameters (patterns) the model can update as it trains.\n",
    "* **Non-trainable parameters** - these parameters aren't updated during training (this is typical when you bring in the already learned patterns from other models during transfer learning).\n",
    "\n",
    "> üìñ **Resource:** For a more in-depth overview of the trainable parameters within a layer, check out [MIT's introduction to deep learning video](https://youtu.be/njKP3FqW3Sk).\n",
    "\n",
    "> üõ† **Exercise:** Try playing around with the number of hidden units in the `Dense` layer (e.g. `Dense(2)`, `Dense(3)`). How does this change the Total/Trainable params? Investigate what's causing the change.\n",
    "\n",
    "For now, all you need to think about these parameters is that their learnable patterns in the data.\n",
    "\n",
    "Let's fit our model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53013bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb06489940>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train, epochs=100, verbose=0) # verbose controls how much gets output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3530576b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 10)                20        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e7513",
   "metadata": {},
   "source": [
    "Alongside summary, you can also view a 2D plot of the model using [`plot_model()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5278f8ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install GraphViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2349f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42caa706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22636a06",
   "metadata": {},
   "source": [
    " ## Visualizing the predictions\n",
    "\n",
    "Now we've got a trained model, let's visualize some predictions.\n",
    "\n",
    "To visualize predictions, it's always a good idea to plot them against the ground truth labels.\n",
    "\n",
    "Often you'll see this in the form of `y_test` vs. `y_pred` (ground truth vs. predictions).\n",
    "\n",
    "First, we'll make some predictions on the test data (`X_test`), remember the model has never seen the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8c4fb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB082D7040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[29.23156]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([24.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01027804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB082D7040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae5c3449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70.46816 ],\n",
       "       [ 75.05001 ],\n",
       "       [ 79.63185 ],\n",
       "       [ 84.2137  ],\n",
       "       [ 88.79554 ],\n",
       "       [ 93.377396],\n",
       "       [ 97.959236],\n",
       "       [102.541084],\n",
       "       [107.12293 ],\n",
       "       [111.704765]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the predictions\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a607374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec210325",
   "metadata": {
    "id": "56euC69rZvNJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train, \n",
    "                     train_labels=y_train, \n",
    "                     test_data=X_test, \n",
    "                     test_labels=y_test, \n",
    "                     predictions=y_preds):\n",
    "    \"\"\"\n",
    "    Plots training data, test data and compares predictions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", label=\"Training data\")\n",
    "    # Plot test data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", label=\"Testing data\")\n",
    "    # Plot the predictions in red (predictions were made on the test data)\n",
    "    plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")\n",
    "    # Show the legend\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cceccf2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "Fug5_B6Ab7Ah",
    "outputId": "8ab057a0-528a-43ad-cdee-078608c58da4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(train_data=X_train,\n",
    "                 train_labels=y_train,\n",
    "                 test_data=X_test,\n",
    "                 test_labels=y_test,\n",
    "                 predictions=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9bd636",
   "metadata": {},
   "source": [
    "## Evaluating predictions\n",
    "\n",
    "Alongisde visualizations, evaulation metrics are your alternative best option for evaluating your model.\n",
    "\n",
    "Depending on the problem you're working on, different models have different evaluation metrics. \n",
    "\n",
    "Two of the main metrics used for regression problems are:\n",
    "* **Mean absolute error (MAE)** - the mean difference between each of the predictions.\n",
    "* **Mean squared error (MSE)** - the squared mean difference between of the predictions (use if larger errors are more detrimental than smaller errors).\n",
    "\n",
    "The lower each of these values, the better.\n",
    "\n",
    "You can also use [`model.evaluate()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate) which will return the loss of the model as well as any metrics setup during the compile step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d7520d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0865 - mae: 3.0865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.086468458175659, 3.086468458175659]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf853c",
   "metadata": {},
   "source": [
    "In our case, since we used MAE for the loss function as well as MAE for the metrics, `model.evaulate()` returns them both.\n",
    "\n",
    "TensorFlow also has built in functions for MSE and MAE.\n",
    "\n",
    "For many evaluation functions, the premise is the same: compare predictions to the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06570f25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([17.62547 , 14.169993, 11.747259, 10.35726 , 10.      , 10.675479,\n",
       "       12.383695, 15.232867, 19.122929, 23.70477 ], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean absolute error\n",
    "mae = tf.metrics.mean_absolute_error(y_true=y_test, \n",
    "                                     y_pred=y_preds)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7f73d",
   "metadata": {},
   "source": [
    "Huh? That's strange, MAE should be a single output.\n",
    "\n",
    "Instead, we get 10 values.\n",
    "\n",
    "This is because our `y_test` and `y_preds` tensors are different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4e6911c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the test label tensor values\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b596b66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70.46816 ],\n",
       "       [ 75.05001 ],\n",
       "       [ 79.63185 ],\n",
       "       [ 84.2137  ],\n",
       "       [ 88.79554 ],\n",
       "       [ 93.377396],\n",
       "       [ 97.959236],\n",
       "       [102.541084],\n",
       "       [107.12293 ],\n",
       "       [111.704765]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the predictions tensor values (notice the extra square brackets)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3167e5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10,), (10, 1))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tensor shapes\n",
    "y_test.shape, y_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88870e",
   "metadata": {},
   "source": [
    "Remember how we discussed dealing with different input and output shapes is one the most common issues you'll come across, this is one of those times.\n",
    "\n",
    "But not to worry.\n",
    "\n",
    "We can fix it using [`squeeze()`](https://www.tensorflow.org/api_docs/python/tf/squeeze), it'll remove the the `1` dimension from our `y_preds` tensor, making it the same shape as `y_test`.\n",
    "\n",
    "> üîë **Note:** If you're comparing two tensors, it's important to make sure they're the right shape(s) (you won't always have to manipulate the shapes, but always be on the look out, *many* errors are the result of mismatched tensors, especially mismatched input and output shapes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc6d2a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape before squeez\n",
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c612462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape after squeeze() (remove dimension)\n",
    "y_preds.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efcf4edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0864677>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate MAE\n",
    "mae = tf.metrics.mean_absolute_error(y_true = y_test,\n",
    "                                     y_pred = y_preds.squeeze())\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16aeb7bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=12.319273>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate MSE\n",
    "mse = tf.metrics.mean_squared_error(y_true = y_test,\n",
    "                                   y_pred = y_preds.squeeze())\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d24ebc",
   "metadata": {},
   "source": [
    "huber_loss = tf.keras.huber_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "091b606a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.6006103>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Huber loss\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "h = Huber()\n",
    "h(y_test, tf.squeeze(y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeaf8b9",
   "metadata": {},
   "source": [
    "Again, it's a good idea to functionize anything you think you might use over again (or find yourself using over and over again).\n",
    "\n",
    "Let's make functions for our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4448982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a fnctions to reuse MAE and MSE\n",
    "def mae(y_true, y_pred):\n",
    "    \n",
    "    return tf.metrics.mean_absolute_error(y_true, tf.squeeze(y_pred))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return tf.metrics.mean_squared_error(y_true, tf.squeeze(y_pred))\n",
    "                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aea298",
   "metadata": {},
   "source": [
    "## Running experiments to improve a model\n",
    "\n",
    "After seeing the evaluation metrics and the predictions your model makes, it's likely you'll want to improve it.\n",
    "\n",
    "Again, there are many different ways you can do this, but 3 of the main ones are:\n",
    "1. **Get more data** - get more examples for your model to train on (more opportunities to learn patterns).\n",
    "2. **Make your model larger (use a more complex model)** - this might come in the form of more layers or more hidden units in each layer.\n",
    "3. **Train for longer** - give your model more of a chance to find the patterns in the data.\n",
    "\n",
    "Since we created our dataset, we could easily make more data but this isn't always the case when you're working with real-world datasets.\n",
    "\n",
    "So let's take a look at how we can improve our model using 2 and 3.\n",
    "\n",
    "To do so, we'll build 3 models and compare their results:\n",
    "1. `model_1` - same as original model, 1 layer, trained for 100 epochs.\n",
    "2. `model_2` - 2 layers, trained for 100 epochs.\n",
    "3. `model_3` - 2 layers, trained for 500 epochs.\n",
    "\n",
    "**Build `model_1`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86f8eaa2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.9024 - mae: 15.9024\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.2837 - mae: 11.2837\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 11.1075 - mae: 11.1075\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.2990 - mae: 9.2990\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.1677 - mae: 10.1677\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.4303 - mae: 9.4303\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.5704 - mae: 8.5704\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 9.0442 - mae: 9.0442\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 18.7517 - mae: 18.7517\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.1142 - mae: 10.1142\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.3980 - mae: 8.3980\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.6639 - mae: 10.6639\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.7977 - mae: 9.7977\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.0103 - mae: 16.0103\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.4068 - mae: 11.4068\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.5393 - mae: 8.5393\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.6348 - mae: 13.6348\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.4629 - mae: 11.4629\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.9148 - mae: 17.9148\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.0494 - mae: 15.0494\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.0216 - mae: 11.0216\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.1558 - mae: 8.1558\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.5138 - mae: 9.5138\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.6617 - mae: 7.6617\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.1859 - mae: 13.1859\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.4211 - mae: 16.4211\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 13.9381 - mae: 13.938 - 0s 4ms/step - loss: 13.1660 - mae: 13.1660\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.2559 - mae: 14.2559\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.0670 - mae: 10.0670\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 16.3409 - mae: 16.3409\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 999us/step - loss: 23.6444 - mae: 23.6444\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.6215 - mae: 7.6215\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.3221 - mae: 9.3221\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.7313 - mae: 13.7313\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.1276 - mae: 11.1276\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.3222 - mae: 13.3222\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.4763 - mae: 9.4763\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.1381 - mae: 10.1381\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.1793 - mae: 10.1793\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 10.9137 - mae: 10.9137\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 998us/step - loss: 7.9063 - mae: 7.9063\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.0914 - mae: 10.0914\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.7006 - mae: 8.7006\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 999us/step - loss: 12.2046 - mae: 12.2046\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.7970 - mae: 13.7970\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4687 - mae: 8.4687\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 9.1330 - mae: 9.1330\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.6190 - mae: 10.6190\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.7503 - mae: 7.7503\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 9.5407 - mae: 9.5407\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.1584 - mae: 9.1584\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 16.3630 - mae: 16.3630\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 14.1299 - mae: 14.1299\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.1247 - mae: 21.1247\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.3961 - mae: 16.3961\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.9806 - mae: 9.9806\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.9606 - mae: 9.9606\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.2209 - mae: 9.2209\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4239 - mae: 8.4239\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.4869 - mae: 9.4869\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.4354 - mae: 11.4354\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.6887 - mae: 11.6887\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.0838 - mae: 7.0838\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 999us/step - loss: 16.9675 - mae: 16.9675\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.4599 - mae: 12.4599\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.0184 - mae: 13.0184\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.0600 - mae: 8.0600\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.1888 - mae: 10.1888\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.3633 - mae: 12.3633\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 999us/step - loss: 9.0516 - mae: 9.0516\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.0378 - mae: 10.0378\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.0516 - mae: 10.0516\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 999us/step - loss: 12.6151 - mae: 12.6151\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.3819 - mae: 10.3819\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.7229 - mae: 9.7229\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.2252 - mae: 11.2252\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.3642 - mae: 8.3642\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.1274 - mae: 9.1274\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.5039 - mae: 19.5039\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.8945 - mae: 14.8945\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.0034 - mae: 9.0034\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.0206 - mae: 13.0206\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.9299 - mae: 7.9299\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.6872 - mae: 7.6872\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.0328 - mae: 10.0328\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.2433 - mae: 9.2433\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 999us/step - loss: 12.0209 - mae: 12.0209\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.6389 - mae: 10.6389\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.2667 - mae: 7.2667\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.7786 - mae: 12.7786\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.3481 - mae: 7.3481\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.7175 - mae: 7.7175\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.1263 - mae: 7.1263\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.6190 - mae: 12.6190\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.0912 - mae: 10.0912\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.3558 - mae: 9.3558\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 12.6834 - mae: 12.6834\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.6762 - mae: 8.6762\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.4693 - mae: 9.4693\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.7067 - mae: 8.7067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb0841ad90>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a model\n",
    "model_1 = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "model_1.compile(loss = tf.keras.losses.mae,\n",
    "                optimizer = tf.keras.optimizers.SGD(),\n",
    "                metrics = ['mae'])\n",
    "\n",
    "# Fit the model\n",
    "model_1.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc96bd7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB0837CD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a plot prediction for model 1\n",
    "y_preds_1 = model_1.predict(X_test)\n",
    "plot_predictions(predictions = y_preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9646646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=18.745327>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=353.57336>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_1 metrics\n",
    "mae_1 = mae(y_test, y_preds_1)\n",
    "mse_1 = mse(y_test, y_preds_1)\n",
    "\n",
    "mae_1, mse_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b86909",
   "metadata": {},
   "source": [
    "**Build `model_2`**\n",
    "\n",
    "This time we'll add an extra dense layer (so now our model will have 2 layers) whilst keeping everything else the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "950f1787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb086818b0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Replicate model_1 and add an extra layer\n",
    "model_2 = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Dense(1) # add a second layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.SGD(),\n",
    "                metrics=['mae'])\n",
    "\n",
    "# Fit the model\n",
    "model_2.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100, verbose=0) # set verbose to 0 for less output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83d1a9ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB08798DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make and plot predictions for model_2\n",
    "y_preds_2 = model_2.predict(X_test)\n",
    "plot_predictions(predictions=y_preds_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9520b1",
   "metadata": {},
   "source": [
    "Woah, that's looking better already! And all it took was an extra layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb12461c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9097328, 5.45877)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_2 metrics\n",
    "mae_2 = mae(y_test, y_preds_2.squeeze()).numpy()\n",
    "mse_2 = mse(y_test, y_preds_2.squeeze()).numpy()\n",
    "mae_2, mse_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec5cfc",
   "metadata": {},
   "source": [
    "**Build `model_3`**\n",
    "\n",
    "For our 3rd model, we'll keep everything the same as `model_2` except this time we'll train for longer (500 epochs instead of 100).\n",
    "\n",
    "This will give our model more of a chance to learn the patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5dc45bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 27.4058 - mse: 1084.1482\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.6339 - mse: 777.9203\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 29.8935 - mse: 1334.8953\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 27.4055 - mse: 1106.8035\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.9463 - mse: 281.1077\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 11.8819 - mse: 168.6621\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.1988 - mse: 151.3508\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 11.0910 - mse: 160.3745\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 40.4763 - mse: 2586.0090\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 27.8687 - mse: 1094.4382\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.2473 - mse: 147.9359\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.2803 - mse: 890.3867\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.9897 - mse: 399.9677\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.9217 - mse: 1049.5515\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.9948 - mse: 450.2580\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.3510 - mse: 80.6206\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 10.8636 - mse: 174.7868\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.5304 - mse: 565.8052\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 10.3469 - mse: 167.7749\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.6985 - mse: 455.7096\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.8985 - mse: 347.1929\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 14.1991 - mse: 285.1767\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 998us/step - loss: 8.7720 - mse: 91.7852\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.0570 - mse: 153.7430\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.6838 - mse: 233.2950\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 26.1877 - mse: 1024.6094\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.7432 - mse: 194.8453\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.8730 - mse: 835.6069\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.2459 - mse: 96.7786\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 29.2641 - mse: 1535.1335\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 53.0224 - mse: 5030.2954\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.9951 - mse: 211.7023\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.6357 - mse: 337.3664\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.6925 - mse: 214.4821\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.2398 - mse: 92.9126\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.6497 - mse: 403.6569\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.0382 - mse: 192.3919\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.1634 - mse: 433.6718\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.1013 - mse: 529.6440\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20.4324 - mse: 610.1325\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 14.9102 - mse: 279.6183\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.2809 - mse: 186.6180\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 10.7333 - mse: 167.0952\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.0260 - mse: 830.4244\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 10.3897 - mse: 128.9549\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 998us/step - loss: 11.7904 - mse: 181.9211\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.6438 - mse: 153.8709\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 17.2335 - mse: 402.8495\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 9.5729 - mse: 99.8336\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.8185 - mse: 260.3669\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.5958 - mse: 154.7956\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.5538 - mse: 1613.0883\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.3541 - mse: 302.5292\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 23.9713 - mse: 859.3983\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.1938 - mse: 805.5450\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.8837 - mse: 170.9834\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.7445 - mse: 198.7014\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.5995 - mse: 102.5890\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.5172 - mse: 216.3367\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.3200 - mse: 208.6370\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.4604 - mse: 428.6392\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 10.6052 - mse: 136.9777\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.4893 - mse: 152.4554\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 24.8450 - mse: 911.7508\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.6761 - mse: 142.7374\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.7809 - mse: 704.4487\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.7136 - mse: 136.0194\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.6397 - mse: 149.2299\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.6914 - mse: 742.1759\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 9.3316 - mse: 166.1627\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 15.4355 - mse: 323.0844\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.7437 - mse: 67.0210\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 8.5643 - mse: 73.51 - 0s 2ms/step - loss: 11.6891 - mse: 183.7296\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.0400 - mse: 908.8988\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 9.5896 - mse: 149.3948\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.4371 - mse: 188.3310\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.6488 - mse: 429.2705\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.0614 - mse: 95.4870\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.9675 - mse: 864.0859\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 26.7463 - mse: 1104.4030\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.6714 - mse: 170.7055\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.0228 - mse: 211.9191\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.4218 - mse: 395.5590\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 7.2629 - mse: 73.0935\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 14.9650 - mse: 312.8361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.2862 - mse: 315.3605\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 19.1086 - mse: 521.2535\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 29.8228 - mse: 1287.1902\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.1742 - mse: 124.1342\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.5240 - mse: 663.8609\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 10.5716 - mse: 161.7467\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 18.3977 - mse: 464.1323\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.4138 - mse: 81.9820\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 17.7380 - mse: 445.7377\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.1144 - mse: 164.0820\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.4346 - mse: 510.5842\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.1593 - mse: 209.9755\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.5653 - mse: 169.4051\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.8827 - mse: 265.4630\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20.2277 - mse: 608.8218\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.4479 - mse: 177.1446\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.4842 - mse: 426.5328\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.0217 - mse: 65.2649\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.5789 - mse: 757.4908\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.8932 - mse: 443.0954\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.2954 - mse: 144.6570\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 25.3749 - mse: 934.2097\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.4621 - mse: 269.8598\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.5238 - mse: 108.7908\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.6722 - mse: 128.8767\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 14.5987 - mse: 295.9862\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 9.5670 - mse: 123.4114\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 17.8092 - mse: 460.3868\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.1782 - mse: 441.2209\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 11.1182 - mse: 155.0962\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 23.3071 - mse: 791.7660\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 9.6144 - mse: 126.1895\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 10.6899 - mse: 140.2946\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.0355 - mse: 78.3508\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 29.6859 - mse: 1315.8650\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.0714 - mse: 70.6648\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 28.3086 - mse: 1233.3853\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32.9014 - mse: 1591.9507\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 19.6291 - mse: 571.6959\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.0095 - mse: 72.1092\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.8056 - mse: 666.4337\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.9812 - mse: 69.5627\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.0585 - mse: 653.6483\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.0107 - mse: 120.8762\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.0502 - mse: 815.7741\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.7537 - mse: 133.0194\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.3052 - mse: 500.9213\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.5833 - mse: 80.1913\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 18.5755 - mse: 506.3341\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.5360 - mse: 148.5977\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.2694 - mse: 472.4461\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 23.1658 - mse: 724.0383\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.1362 - mse: 139.5299\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.9181 - mse: 144.6080\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.4732 - mse: 389.2470\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.4208 - mse: 91.7413\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 998us/step - loss: 36.9540 - mse: 2404.8645\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 25.5820 - mse: 927.2339\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.5392 - mse: 144.3266\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26.6058 - mse: 957.1338\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.7248 - mse: 109.2954\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 15.6172 - mse: 311.2310\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 18.3065 - mse: 464.7855\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.1994 - mse: 106.4584\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.4964 - mse: 66.4456\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 18.3374 - mse: 485.9980\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.2895 - mse: 130.5616\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 29.6425 - mse: 1268.4641\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.5556 - mse: 201.2336\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.4537 - mse: 347.2169\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.0174 - mse: 438.8493\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32.8218 - mse: 1744.8152\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 10.7038 - mse: 151.0814\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.9054 - mse: 97.7705\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 22.1321 - mse: 709.4399\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.7113 - mse: 202.9613\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.5734 - mse: 670.2867\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 19.2485 - mse: 533.4080\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.0156 - mse: 177.6828\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.6187 - mse: 179.8634\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.5908 - mse: 681.9664\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26.2851 - mse: 1048.8455\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.8525 - mse: 118.2892\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 22.5631 - mse: 787.0594\n",
      "Epoch 170/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 1ms/step - loss: 10.1499 - mse: 195.9051\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 18.0464 - mse: 503.4531\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.8377 - mse: 1305.7992\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.5280 - mse: 432.1444\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.2115 - mse: 199.9692\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 27.5839 - mse: 1097.8187\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.2680 - mse: 77.5372\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2580 - mse: 108.9075\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.1440 - mse: 466.0514\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.5995 - mse: 147.8543\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.8992 - mse: 100.6932\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.4015 - mse: 438.6036\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 9.1552 - mse: 95.46 - 0s 2ms/step - loss: 11.0089 - mse: 157.1596\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.7027 - mse: 203.9956\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 30.4062 - mse: 1387.3848\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.5557 - mse: 98.6847\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.9905 - mse: 369.3700\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.5579 - mse: 85.4846\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 28.7339 - mse: 1175.7061\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.1689 - mse: 271.3447\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 18.3101 - mse: 512.5546\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 13.7376 - mse: 264.9675\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.7104 - mse: 261.0630\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 28.5842 - mse: 1135.1305\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 7.0707 - mse: 78.6824\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.0550 - mse: 74.9454\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.0067 - mse: 709.4540\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.8443 - mse: 643.6119\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.4713 - mse: 236.5509\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.9099 - mse: 460.3528\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.7493 - mse: 282.2667\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.4687 - mse: 41.0883\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 13.7005 - mse: 293.8445\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.4142 - mse: 139.9066\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.9796 - mse: 656.2624\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.5470 - mse: 127.9145\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.7256 - mse: 195.6239\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.3772 - mse: 317.1676\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.8579 - mse: 323.1452\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 14.9706 - mse: 345.7375\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.8998 - mse: 467.4643\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.8327 - mse: 145.8636\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.3352 - mse: 507.7762\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.0383 - mse: 307.6408\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.5874 - mse: 293.5643\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.3015 - mse: 799.4149\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.3613 - mse: 278.0948\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.8517 - mse: 136.3091\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.5451 - mse: 206.5810\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.9472 - mse: 37.4063\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.1130 - mse: 57.5580\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 35.4567 - mse: 2108.1006\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 34.8634 - mse: 1966.9600\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.9846 - mse: 119.7548\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.7004 - mse: 318.5026\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 16.7196 - mse: 360.6417\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.9329 - mse: 369.8892\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.1644 - mse: 369.5818\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 998us/step - loss: 13.9324 - mse: 282.1422\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.0504 - mse: 457.4228\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.6120 - mse: 314.6218\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2041 - mse: 670.7589\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.2732 - mse: 913.5802\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.3176 - mse: 387.7157\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.2729 - mse: 66.5098\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 16.9688 - mse: 403.1293\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.1225 - mse: 69.7516\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.2058 - mse: 118.9100\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.0961 - mse: 86.7115\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 17.0538 - mse: 438.0265\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.8627 - mse: 107.8191\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.1711 - mse: 273.2420\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 8.7886 - mse: 104.8365\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.8161 - mse: 541.4709\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.0531 - mse: 275.3817\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.6831 - mse: 290.1829\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 15.8045 - mse: 370.7156\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.6810 - mse: 421.4948\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.2367 - mse: 251.7198\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.5070 - mse: 288.7693\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.2322 - mse: 793.2505\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.3009 - mse: 119.0355\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 36.6568 - mse: 2195.6155\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 21.8205 - mse: 667.9622\n",
      "Epoch 254/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 7.2792 - mse: 77.8094\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 24.7126 - mse: 882.9651\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 9.9454 - mse: 137.306 - 0s 2ms/step - loss: 12.4220 - mse: 220.3513\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.5823 - mse: 164.0438\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 14.4883 - mse: 322.3508\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.6132 - mse: 98.6931\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 43.0580 - mse: 2975.2290\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 18.4611 - mse: 493.6656\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.8820 - mse: 94.3407\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.7211 - mse: 268.5680\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.0154 - mse: 653.9620\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 19.3730 - mse: 545.6235\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.4735 - mse: 234.7202\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.5302 - mse: 112.0906\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.6453 - mse: 670.2257\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 33.1785 - mse: 1653.7346\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.0833 - mse: 148.7349\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.1012 - mse: 303.1721\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 26.1372 - mse: 944.6783\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.1751 - mse: 228.8029\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.3272 - mse: 282.3030\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 29.3775 - mse: 1229.1169\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3329 - mse: 104.3414\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 31.1362 - mse: 1360.7686\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.3016 - mse: 234.6619\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.4103 - mse: 416.9119\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 21.9118 - mse: 716.7850\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.1500 - mse: 745.2045\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.7429 - mse: 90.5224\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.1429 - mse: 87.9049\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.9434 - mse: 936.1583\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.6958 - mse: 299.3606\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.8926 - mse: 81.2997\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.5352 - mse: 862.2885\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20.1721 - mse: 605.2863\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.9658 - mse: 244.8305\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.5391 - mse: 366.5722\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.8017 - mse: 414.8180\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.4642 - mse: 213.4542\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.2711 - mse: 318.1686\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.7179 - mse: 780.8715\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.9234 - mse: 458.5727\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.1742 - mse: 59.0634\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.9440 - mse: 254.2859\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 23.1530 - mse: 785.9871\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.7331 - mse: 469.9261\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.9824 - mse: 65.0501\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 25.1857 - mse: 898.2772\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.9025 - mse: 116.0075\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.7668 - mse: 461.9366\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.0002 - mse: 173.9247\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.9191 - mse: 271.8379\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.4033 - mse: 94.4443\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.6094 - mse: 279.1168\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.4404 - mse: 78.3838\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.4642 - mse: 138.8856\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.7099 - mse: 198.2183\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 13.2814 - mse: 254.2446\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 29.9763 - mse: 1241.6277\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.6304 - mse: 104.4099\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.9106 - mse: 239.5552\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.7669 - mse: 820.7440\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 16.3936 - mse: 404.4792\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.0758 - mse: 606.4598\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.9367 - mse: 81.3764\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.9731 - mse: 479.6086\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.2375 - mse: 174.0453\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.3338 - mse: 110.8493\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.0621 - mse: 39.6195\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.5109 - mse: 802.4886\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.8309 - mse: 57.6745\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.3863 - mse: 384.4402\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.5019 - mse: 78.7650\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 20.0573 - mse: 568.8239\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.7661 - mse: 265.8646\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 16.8282 - mse: 430.9921\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.0514 - mse: 81.9922\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.4846 - mse: 704.3916\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.2880 - mse: 236.5700\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.8117 - mse: 222.0920\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.3600 - mse: 169.8893\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.4833 - mse: 276.8587\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32.2171 - mse: 1416.1726\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.4477 - mse: 180.3261\n",
      "Epoch 338/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 999us/step - loss: 19.6832 - mse: 573.4708\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 35.0762 - mse: 1849.1559\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.4192 - mse: 181.4145\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.7625 - mse: 156.3231\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.9500 - mse: 188.5578\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 9.3943 - mse: 145.3949\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 5.6071 - mse: 44.6760\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 37.4876 - mse: 2291.7803\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 16.8830 - mse: 411.1354\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.8748 - mse: 281.5984\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 8.1960 - mse: 162.6964\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.5568 - mse: 261.1368\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.4354 - mse: 333.2079\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32.9626 - mse: 1524.7930\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 14.2040 - mse: 282.7505\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.9196 - mse: 374.1913\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 19.0878 - mse: 534.9767\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 34.1178 - mse: 1782.7047\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6798 - mse: 90.9119\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.2287 - mse: 974.0955\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 22.6759 - mse: 742.0947\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.8765 - mse: 198.2594\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 21.4709 - mse: 687.3273\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 20.6073 - mse: 616.7955\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 7.0611 - mse: 69.3817\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 25.8117 - mse: 994.8207\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 32.2247 - mse: 1527.6089\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.0205 - mse: 161.6851\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.6722 - mse: 227.5728\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.4171 - mse: 1294.6041\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.5020 - mse: 236.3137\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.9909 - mse: 330.8540\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 14.6580 - mse: 308.0733\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 23.3672 - mse: 789.0350\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.1025 - mse: 290.2273\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9.2586 - mse: 140.7055\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.6648 - mse: 194.1148\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.0041 - mse: 237.9866\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.8863 - mse: 324.7534\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.7932 - mse: 297.2056\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.2751 - mse: 418.5629\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 20.8307 - mse: 596.8128\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 33.5318 - mse: 1684.8971\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.2166 - mse: 100.6387\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.0960 - mse: 282.0389\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.3999 - mse: 126.0927\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.1283 - mse: 72.4400\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 10.9390 - mse: 249.4379\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 19.7654 - mse: 601.5789\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.8625 - mse: 896.3714\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.7422 - mse: 128.0919\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.9488 - mse: 48.1570\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 24.4401 - mse: 881.1058\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.9771 - mse: 73.2798\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 16.3250 - mse: 379.2600\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.0917 - mse: 76.7832\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 11.0963 - mse: 204.7653\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.9601 - mse: 336.1571\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6462 - mse: 103.3248\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.7654 - mse: 136.5435\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 14.5992 - mse: 320.4543\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.3166 - mse: 280.6894\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.9080 - mse: 741.3456\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.8654 - mse: 348.6730\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.4970 - mse: 115.5855\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.3957 - mse: 191.0405\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.2556 - mse: 207.5720\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.3392 - mse: 68.2857\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.4602 - mse: 460.2060\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.4627 - mse: 284.7281\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 20.7294 - mse: 662.1494\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 31.3339 - mse: 1522.7717\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 9.2542 - mse: 221.9491\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 14.8621 - mse: 285.8413\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.7182 - mse: 721.0385\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.6615 - mse: 248.9699\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.0687 - mse: 88.3709\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.2201 - mse: 239.8953\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 27.4244 - mse: 1037.9574\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.6407 - mse: 207.4824\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 12.8230 - mse: 236.7906\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.8836 - mse: 374.5479\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 24.7510 - mse: 849.5324\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.3753 - mse: 444.1840\n",
      "Epoch 422/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 7.8241 - mse: 157.5332\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 25.3789 - mse: 907.6134\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.1031 - mse: 369.3296\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.1643 - mse: 75.3718\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20.3318 - mse: 572.0366\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.3283 - mse: 84.9744\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.9962 - mse: 288.4222\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.7869 - mse: 192.7745\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.4007 - mse: 248.2727\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.6153 - mse: 218.3400\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.4582 - mse: 239.2202\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.3851 - mse: 296.6790\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 30.3986 - mse: 1296.5627\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.5052 - mse: 283.6821\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.8810 - mse: 1216.2332\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.5916 - mse: 209.5437\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 12.7378 - mse: 264.4514\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 33.6754 - mse: 1570.2357\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.0962 - mse: 294.7130\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 17.4813 - mse: 487.0568\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.3049 - mse: 750.9300\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.5841 - mse: 784.1396\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.0008 - mse: 203.7588\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.9175 - mse: 316.4787\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 17.9979 - mse: 515.1707\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.4482 - mse: 51.4485\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 10.0527 - mse: 253.4832\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 14.0052 - mse: 277.3351\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.7782 - mse: 424.4224\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.2937 - mse: 303.4475\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 30.6192 - mse: 1329.8262\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6541 - mse: 159.0381\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 28.1428 - mse: 1104.4135\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.0017 - mse: 119.3277\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.3933 - mse: 289.6396\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.0242 - mse: 318.3532\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 16.5653 - mse: 439.8002\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 26.8566 - mse: 1033.5481\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.4852 - mse: 251.1166\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 12.4784 - mse: 272.9072\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.3186 - mse: 245.2334\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 29.5524 - mse: 1219.8655\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.4664 - mse: 22.0146\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.2136 - mse: 352.9364\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.8327 - mse: 651.1586\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.5108 - mse: 1421.0046\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.0598 - mse: 236.2268\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 12.8372 - mse: 278.2050\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.2398 - mse: 14.0654\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 16.6964 - mse: 372.9562\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 13.3883 - mse: 253.4231\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 15.2771 - mse: 396.6104\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 11.7448 - mse: 305.3253\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.4113 - mse: 397.1608\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.8785 - mse: 273.7974\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.6702 - mse: 1309.4620\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 8.5880 - mse: 188.9075\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 10.7384 - mse: 261.5207\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 17.9051 - mse: 484.4149\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 15.8094 - mse: 368.1040\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 21.3054 - mse: 698.1629\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 25.3845 - mse: 955.1268\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 23.9816 - mse: 811.2802\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.7734 - mse: 53.7612\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 20.0011 - mse: 565.2890\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 999us/step - loss: 14.0419 - mse: 286.2682\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.6088 - mse: 1334.5725\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.9409 - mse: 237.4729\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.7352 - mse: 252.5675\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.6139 - mse: 837.7731\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 20.5365 - mse: 592.2194\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.9942 - mse: 48.0281\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12.7987 - mse: 247.7650\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 13.3772 - mse: 239.0899\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 12.6727 - mse: 241.5636\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 17.6192 - mse: 515.3856\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 23.5629 - mse: 816.5515\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.3755 - mse: 136.1010\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 14.6316 - mse: 295.2642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb0b7c0af0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed \n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a model\n",
    "model_3 = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "model_3.compile(loss = tf.keras.losses.mae,\n",
    "                optimizer = tf.keras.optimizers.SGD(),\n",
    "                metrics = ['mse'])\n",
    "\n",
    "# Fit the model\n",
    "model_3.fit(X_train, y_train, epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f1f68a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EB0806C700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make and plot predictions for model_3\n",
    "y_preds_3 = model_3.predict(X_test)\n",
    "\n",
    "plot_predictions(predictions=y_preds_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90da8057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=68.713615>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4808.0273>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_3 evaluation metics\n",
    "mae_3 = mae(y_test, y_preds_3)\n",
    "mse_3 = mse(y_test, y_preds_3)\n",
    "\n",
    "mae_3, mse_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5a300",
   "metadata": {},
   "source": [
    "## Comparing results\n",
    "\n",
    "Now we've got results for 3 similar but slightly different results, let's compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8254f17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_results = [[\"model_1\", mae_1.numpy(), mse_1.numpy()],\n",
    "                 [\"model_2\", mae_2, mse_2],\n",
    "                 [\"model_3\", mae_3.numpy(), mae_3.numpy()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a515ca10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model_1</td>\n",
       "      <td>18.745327</td>\n",
       "      <td>353.573364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model_2</td>\n",
       "      <td>1.909733</td>\n",
       "      <td>5.458770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model_3</td>\n",
       "      <td>68.713615</td>\n",
       "      <td>68.713615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model        mae         mse\n",
       "0  model_1  18.745327  353.573364\n",
       "1  model_2   1.909733    5.458770\n",
       "2  model_3  68.713615   68.713615"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results = pd.DataFrame(model_results, columns=[\"model\", \"mae\", \"mse\"])\n",
    "\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7bd860a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43f5ae",
   "metadata": {},
   "source": [
    "From our experiments, it looks like `model_2` performed the best.\n",
    "\n",
    "And now, you might be thinking, \"wow, comparing models is tedious...\" and it definitely can be, we've only compared 3 models here. \n",
    "\n",
    "But this is part of what machine learning modelling is about, trying many different combinations of models and seeing which performs best.\n",
    "\n",
    "Each model you build is a small experiment. \n",
    "\n",
    "> üîë **Note:** One of your main goals should be to minimize the time between your experiments. The more experiments you do, the more things you'll figure out which don't work and in turn, get closer to figuring out what does work. Remember the machine learning practitioner's motto: \"experiment, experiment, experiment\".\n",
    "\n",
    "Another thing you'll also find is what you thought may work (such as training a model for longer) may not always work and the exact opposite is also often the case.\n",
    "\n",
    "## Tracking your experiments\n",
    "\n",
    "One really good habit to get into is tracking your modelling experiments to see which perform better than others.\n",
    "\n",
    "We've done a simple version of this above (keeping the results in different variables).\n",
    "\n",
    "> üìñ **Resource:** But as you build more models, you'll want to look into using tools such as:\n",
    "* [**TensorBoard**](https://tensorboard.dev/) - a component of the TensorFlow library to help track modelling experiments (we'll see this later).\n",
    "* [**Weights & Biases**](https://www.wandb.com/) - a tool for tracking all kinds of machine learning experiments (the good news for Weights & Biases is it plugs into TensorBoard)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd580805",
   "metadata": {},
   "source": [
    "## Saving a model\n",
    "\n",
    "Once you've trained a model and found one which performs to your liking, you'll probably want to save it for use elsewhere (like a web application or mobile device).\n",
    "\n",
    "You can save a TensorFlow/Keras model using [`model.save()`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model).\n",
    "\n",
    "There are two ways to save a model in TensorFlow:\n",
    "1. The [SavedModel format](https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format) (default).\n",
    "2. The [HDF5 format](https://www.tensorflow.org/tutorials/keras/save_and_load#hdf5_format).\n",
    "\n",
    "The main difference between the two is the SavedModel is automatically able to save custom objects (such as special layers) without additional modifications when loading the model back in.\n",
    "\n",
    "Which one should you use?\n",
    "\n",
    "It depends on your situation but the SavedModel format will suffice most of the time.\n",
    "\n",
    "Both methods use the same method call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "913ba2c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: practice_model_2\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save a model using the SavedModel format\n",
    "model_2.save(\"practice_model_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8a262",
   "metadata": {},
   "source": [
    "Now let's save the model in the HDF5 format, we'll use the same method but with a different filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06be53f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save a model using the HDF5 format\n",
    "model_2.save(\"best_mode_2.h5\") # note the addition of '.h5' on the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca4f58",
   "metadata": {},
   "source": [
    "## Loading a model\n",
    "\n",
    "We can load a saved model using the [`load_model()`](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model) method.\n",
    "\n",
    "Loading a model for the different formats (SavedModel and HDF5) is the same (as long as the pathnames to the particular formats are correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15cb01a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load a model from the SavedModel format\n",
    "loaded_saved_model = tf.keras.models.load_model(\"best_mode_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "61fd201c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " loaded_saved_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841283b",
   "metadata": {},
   "source": [
    "Now let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aba7ba8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare model_2 with the SavedModel version (should return True)\n",
    "model_2_preds = model_2.predict(X_test)\n",
    "\n",
    "saved_model_preds = loaded_saved_model.predict(X_test)\n",
    "\n",
    "# check they are equal or not\n",
    "model_2_preds == saved_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "52229e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae(y_test, saved_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190fe1b",
   "metadata": {},
   "source": [
    "## Downloading a model (from Google Colab)\n",
    "\n",
    "Say you wanted to get your model from Google Colab to your local machine, you can do one of the following things:\n",
    "* Right click on the file in the files pane and click 'download'.\n",
    "* Use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d595ae17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Download the model (or any file) from Google Colab\n",
    "# from google.colab import files\n",
    "# files.download(\"best_model_HDF5_format.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015df39c",
   "metadata": {},
   "source": [
    "## A larger example\n",
    "\n",
    "Alright, we've seen the fundamentals of building neural network regression models in TensorFlow.\n",
    "\n",
    "Let's step it up a notch and build a model for a more feature rich dataset.\n",
    "\n",
    "More specifically we're going to try predict the cost of medical insurance for individuals based on a number of different parameters such as, `age`, `sex`, `bmi`, `children`, `smoking_status` and `residential_region`.\n",
    "\n",
    "To do, we'll leverage the pubically available [Medical Cost dataset](https://www.kaggle.com/mirichoi0218/insurance) available from Kaggle and [hosted on GitHub](https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/insurance.csv).\n",
    "\n",
    "> üîë **Note:** When learning machine learning paradigms, you'll often go through a series of foundational techniques and then practice them by working with open-source datasets and examples. Just as we're doing now, learn foundations, put them to work with different problems. Every time you work on something new, it's a good idea to search for something like \"problem X example with Python/TensorFlow\" where you substitute X for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7ced8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b476784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the insurance dataset\n",
    "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7e5d7c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dede2182",
   "metadata": {},
   "source": [
    "We're going to have to turn the non-numerical columns into numbers (because a neural network can't handle non-numerical inputs).\n",
    "\n",
    "To do so, we'll use the [`get_dummies()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) method in pandas.\n",
    "\n",
    "It converts categorical variables (like the `sex`, `smoker` and `region` columns) into numerical variables using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6c15ce8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>charges</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>smoker_no</th>\n",
       "      <th>smoker_yes</th>\n",
       "      <th>region_northeast</th>\n",
       "      <th>region_northwest</th>\n",
       "      <th>region_southeast</th>\n",
       "      <th>region_southwest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     bmi  children      charges  sex_female  sex_male  smoker_no  \\\n",
       "0   19  27.900         0  16884.92400           1         0          0   \n",
       "1   18  33.770         1   1725.55230           0         1          1   \n",
       "2   28  33.000         3   4449.46200           0         1          1   \n",
       "3   33  22.705         0  21984.47061           0         1          1   \n",
       "4   32  28.880         0   3866.85520           0         1          1   \n",
       "\n",
       "   smoker_yes  region_northeast  region_northwest  region_southeast  \\\n",
       "0           1                 0                 0                 0   \n",
       "1           0                 0                 0                 1   \n",
       "2           0                 0                 0                 1   \n",
       "3           0                 0                 1                 0   \n",
       "4           0                 0                 1                 0   \n",
       "\n",
       "   region_southwest  \n",
       "0                 1  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn all categories into numbers\n",
    "insurance_one_hot = pd.get_dummies(insurance)\n",
    "insurance_one_hot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b82dc",
   "metadata": {},
   "source": [
    "Now we'll split data into features (`X`) and labels (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "750e0325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1338, 11), (1338,))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = insurance_one_hot.drop(\"charges\", axis=1)\n",
    "y = insurance_one_hot[\"charges\"]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc0642",
   "metadata": {},
   "source": [
    "And create training and test sets. We could do this manually, but to make it easier, we'll leverage the already available [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function available from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9b26ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3c1deab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1070, 11), (268, 11), (1070,), (268,))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1ce84",
   "metadata": {},
   "source": [
    "Now we can build and fit a model (we'll make it the same as `model_2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6d051f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1070, 11])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "291b2995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_57 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 8484.2939 - mae: 8484.2939\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 6574.1372 - mae: 6574.1372\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 5852.4258 - mae: 5852.4258\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 4635.2241 - mae: 4635.2241\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3868.5911 - mae: 3868.5911\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3911.0381 - mae: 3911.0381\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3972.2444 - mae: 3972.2444\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3853.7822 - mae: 3853.7822\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3758.8975 - mae: 3758.8975\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3873.0449 - mae: 3873.0449\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 4018.6677 - mae: 4018.6677\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3847.4209 - mae: 3847.4209\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3850.0457 - mae: 3850.0457\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 4177.1836 - mae: 4177.1836\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3881.2178 - mae: 3881.2178\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3916.9194 - mae: 3916.9194\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3875.5981 - mae: 3875.5981\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3895.6470 - mae: 3895.6470\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3937.9202 - mae: 3937.9202\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3850.1514 - mae: 3850.1514\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 4111.8579 - mae: 4111.8579\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3939.0093 - mae: 3939.0093\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 4017.3308 - mae: 4017.3308\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3665.3186 - mae: 3665.3186\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3871.5098 - mae: 3871.5098\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3618.7290 - mae: 3618.7290\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3798.6660 - mae: 3798.6660\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3662.8411 - mae: 3662.8411\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3638.1899 - mae: 3638.1899\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3989.3594 - mae: 3989.3594\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3771.6584 - mae: 3771.6584\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 3732.6157 - mae: 3732.6157A: 0s - loss: 3850.2693 - mae: 3850.269\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3669.1177 - mae: 3669.1177\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3814.7168 - mae: 3814.7168\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3793.9180 - mae: 3793.9180\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3760.4868 - mae: 3760.4868\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3760.8794 - mae: 3760.8794\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3984.7019 - mae: 3984.7019\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3645.0391 - mae: 3645.0391\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3735.8823 - mae: 3735.8823\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3702.7144 - mae: 3702.7144\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3929.7961 - mae: 3929.7961\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3649.8970 - mae: 3649.8970\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3672.8457 - mae: 3672.8457\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3623.8386 - mae: 3623.8386\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3746.8127 - mae: 3746.8127\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3749.4858 - mae: 3749.4858\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3750.9446 - mae: 3750.9446\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3647.0061 - mae: 3647.0061\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3622.5132 - mae: 3622.5132\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3636.7405 - mae: 3636.7405\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3696.1152 - mae: 3696.1152\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3632.5552 - mae: 3632.5552\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3665.9966 - mae: 3665.9966\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3772.0596 - mae: 3772.0596\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3759.3611 - mae: 3759.3611\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3756.1113 - mae: 3756.1113\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3618.6414 - mae: 3618.6414\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3541.1353 - mae: 3541.1353\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3618.2551 - mae: 3618.2551\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3776.2539 - mae: 3776.2539\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3645.3018 - mae: 3645.3018\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3597.6455 - mae: 3597.6455\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3605.7556 - mae: 3605.7556\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3573.1121 - mae: 3573.1121\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3653.4272 - mae: 3653.4272\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3667.0315 - mae: 3667.0315\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3973.7136 - mae: 3973.7136\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3637.2373 - mae: 3637.2373\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3882.3164 - mae: 3882.3164\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3720.1992 - mae: 3720.1992\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3704.2981 - mae: 3704.2981\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3692.5178 - mae: 3692.5178\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 2ms/step - loss: 3686.8936 - mae: 3686.8936\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3666.4624 - mae: 3666.4624\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3670.6782 - mae: 3670.6782\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3596.1414 - mae: 3596.1414\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3731.8479 - mae: 3731.8479\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3978.2336 - mae: 3978.2336\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3672.6899 - mae: 3672.6899\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3712.2166 - mae: 3712.2166\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3840.4839 - mae: 3840.4839\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3852.8979 - mae: 3852.8979\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3678.3154 - mae: 3678.3154\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3683.2271 - mae: 3683.2271\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3562.2346 - mae: 3562.2346\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3857.8323 - mae: 3857.8323\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3643.1938 - mae: 3643.1938\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3691.7637 - mae: 3691.7637\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3657.1248 - mae: 3657.1248\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3602.4307 - mae: 3602.4307\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3763.1025 - mae: 3763.1025\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3835.4009 - mae: 3835.4009\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 4019.5911 - mae: 4019.5911\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3769.6621 - mae: 3769.6621\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 3624.1348 - mae: 3624.1348\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3701.3127 - mae: 3701.3127\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3574.2581 - mae: 3574.2581\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3806.7290 - mae: 3806.7290\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3649.2805 - mae: 3649.2805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb1204d160>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build a model\n",
    "insurance_model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "insurance_model.compile(loss = tf.keras.losses.mae,\n",
    "                        optimizer = tf.keras.optimizers.Adam(lr=0.1),\n",
    "                        metrics = [\"mae\"])\n",
    "\n",
    "# Fit a model\n",
    "insurance_model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ddf3cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 889us/step - loss: 3486.8049 - mae: 3486.8049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3486.804931640625, 3486.804931640625]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results of the insurance model\n",
    "insurance_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115d5f9",
   "metadata": {},
   "source": [
    "Our model didn't perform very well, let's try a bigger model.\n",
    "\n",
    "We'll try 3 things:\n",
    "- Increasing the number of layers (2 -> 3).\n",
    "- Increasing the number of units in each layer (except for the output layer).\n",
    "- Changing the optimizer (from SGD to Adam).\n",
    "\n",
    "Everything else will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ad1405c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_65 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 13073.3975 - mae: 13073.3975\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 10220.4141 - mae: 10220.4141\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 7427.3428 - mae: 7427.3428\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 7281.5991 - mae: 7281.5991\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 7206.6201 - mae: 7206.6201\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 7107.5156 - mae: 7107.5156\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 7006.1816 - mae: 7006.1816\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6879.4946 - mae: 6879.4946\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 6747.8857 - mae: 6747.8857\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 6654.1157 - mae: 6654.1157\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6633.1704 - mae: 6633.1704\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6583.1929 - mae: 6583.1929\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6521.1875 - mae: 6521.1875\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6488.0142 - mae: 6488.0142\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6457.2769 - mae: 6457.2769\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6406.9214 - mae: 6406.9214\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6364.1201 - mae: 6364.1201\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6319.9189 - mae: 6319.9189\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6268.3491 - mae: 6268.3491\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6215.2817 - mae: 6215.2817\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6129.4585 - mae: 6129.4585\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 6062.4321 - mae: 6062.4321\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 5959.1338 - mae: 5959.1338\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 5830.2217 - mae: 5830.2217\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 5683.4204 - mae: 5683.4204\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 5458.4580 - mae: 5458.4580\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 5165.3760 - mae: 5165.3760\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 4737.2407 - mae: 4737.2407\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 4189.8164 - mae: 4189.8164\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3917.5244 - mae: 3917.5244\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3878.7324 - mae: 3878.7324\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3859.5408 - mae: 3859.5408\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3819.8608 - mae: 3819.8608\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3905.5471 - mae: 3905.5471\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3818.3225 - mae: 3818.3225\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3806.6577 - mae: 3806.6577\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3779.8872 - mae: 3779.8872\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3776.9744 - mae: 3776.9744\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3777.3887 - mae: 3777.3887\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3764.1570 - mae: 3764.1570\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3825.1064 - mae: 3825.1064\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3856.0776 - mae: 3856.0776\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3785.8347 - mae: 3785.8347\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3811.4209 - mae: 3811.4209\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3734.4739 - mae: 3734.4739\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3715.1072 - mae: 3715.1072\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3756.2378 - mae: 3756.2378\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3715.3340 - mae: 3715.3340\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3700.9749 - mae: 3700.9749\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3711.4646 - mae: 3711.4646\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3682.9253 - mae: 3682.9253\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3665.8855 - mae: 3665.8855\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3695.9756 - mae: 3695.9756\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 3698.5352 - mae: 3698.5352\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3715.3428 - mae: 3715.3428\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3645.4414 - mae: 3645.4414\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3652.7554 - mae: 3652.7554\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3655.2537 - mae: 3655.2537\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3628.4976 - mae: 3628.4976\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3650.5957 - mae: 3650.5957\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3629.7466 - mae: 3629.7466\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3608.6040 - mae: 3608.6040\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3611.5571 - mae: 3611.5571\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3603.0901 - mae: 3603.0901\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 3572.9211 - mae: 3572.9211\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3635.4419 - mae: 3635.4419\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3600.4800 - mae: 3600.4800\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3597.0164 - mae: 3597.0164\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3564.0571 - mae: 3564.0571\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3569.6277 - mae: 3569.6277\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3542.1936 - mae: 3542.1936\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3547.7769 - mae: 3547.7769\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3537.5540 - mae: 3537.5540\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3518.1348 - mae: 3518.1348\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 2ms/step - loss: 3532.6777 - mae: 3532.6777\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3531.5601 - mae: 3531.5601\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 3571.2722 - mae: 3571.2722\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3521.7271 - mae: 3521.7271\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3560.5342 - mae: 3560.5342\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3530.7593 - mae: 3530.7593\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3542.1323 - mae: 3542.1323\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3539.8235 - mae: 3539.8235\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3517.6492 - mae: 3517.6492\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3510.4861 - mae: 3510.4861\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3492.6582 - mae: 3492.6582\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3511.6938 - mae: 3511.6938\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3514.4290 - mae: 3514.4290\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3541.6855 - mae: 3541.6855\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3505.0542 - mae: 3505.0542\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3525.4375 - mae: 3525.4375\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3554.9226 - mae: 3554.9226\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3595.0386 - mae: 3595.0386\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3523.4465 - mae: 3523.4465\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3524.0417 - mae: 3524.0417\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3561.0732 - mae: 3561.0732\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3512.7012 - mae: 3512.7012\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3522.2217 - mae: 3522.2217\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3526.1123 - mae: 3526.1123\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3497.7678 - mae: 3497.7678\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 3490.8347 - mae: 3490.8347\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "insurance_model_2 = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(50),\n",
    "    tf.keras.layers.Dense(25),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Compile a model\n",
    "insurance_model_2.compile(loss = tf.keras.losses.mae,\n",
    "                        optimizer = tf.keras.optimizers.Adam(),\n",
    "                        metrics = [\"mae\"])\n",
    "\n",
    "# Fit a model\n",
    "history = insurance_model_2.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f500ff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1000us/step - loss: 3167.6304 - mae: 3167.6304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3167.63037109375, 3167.63037109375]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our larger model\n",
    "insurance_model_2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b3887",
   "metadata": {},
   "source": [
    "Much better! Using a larger model and the Adam optimizer results in almost half the error as the previous model.\n",
    "\n",
    "> üîë **Note:** For many problems, the [Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) is a great starting choice. See Andrei Karpathy's \"Adam is safe\" point from [*A Recipe for Training Neural Networks*](http://karpathy.github.io/2019/04/25/recipe/) for more. \n",
    "\n",
    "Let's check out the loss curves of our model, we should see a downward trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4ec5e33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAApnUlEQVR4nO3de3zcdZ3v8ddnZnJPmrZpekt6py30Ai1tsYAWFNeCN2AVLeulIsguh1X0HFHQ46rHI6vbPcuqi6ysKMULiIDCKlCgslzcQptesLSlF3tNb+ktaXPPzHzOH/NLHUpakkySX5t5Px+PPDLznd/vN59vm+bd7+83Mx9zd0RERLorEnYBIiJyZlOQiIhIRhQkIiKSEQWJiIhkREEiIiIZiYVdQF8bMmSIjx07NuwyRETOKCtXrjzo7uUdPZZ1QTJ27FiqqqrCLkNE5IxiZjtO9phObYmISEYUJCIikhEFiYiIZCTrrpGIiHRXW1sb1dXVNDc3h11Kr8nPz6eyspKcnJxO76MgERHppOrqakpKShg7dixmFnY5Pc7dOXToENXV1YwbN67T++nUlohIJzU3N1NWVtYvQwTAzCgrK+vyiktBIiLSBf01RNp1Z34Kkk7a8MoSlv3HLSTi8bBLERE5rShIOqlu8zIu3H0fDfV1YZciIlmsuLg47BLeREHSSZZXAkBTfW24hYiInGYUJJ0ULRwAQPOx2nALEREh9QqrW2+9lWnTpjF9+nR+9atfAbB3717mzZvHjBkzmDZtGi+++CKJRIJPfepTx7e98847e7QWvfy3k3IKUiuS5gad2hIR+OZ/rmP9nqM9eswpIwfw9Q9M7dS2jz76KGvWrOHVV1/l4MGDzJkzh3nz5vHLX/6S+fPn89WvfpVEIkFjYyNr1qxh9+7dvPbaawDU1tb2aN1akXRSTuFAAFobj4VbiIgI8NJLL3HttdcSjUYZNmwYl1xyCStWrGDOnDn89Kc/5Rvf+AZr166lpKSE8ePHs3XrVj772c/y1FNPMWDAgB6tRSuSTsoLTm3Fm2rDLURETgudXTn0FnfvcHzevHm88MIL/P73v+cTn/gEt956K5/85Cd59dVXWbJkCXfddRcPPfQQP/nJT3qsFq1IOim/uBSAeJNWJCISvnnz5vGrX/2KRCLBgQMHeOGFF7jgggvYsWMHQ4cO5TOf+QzXX389q1at4uDBgySTST70oQ/xrW99i1WrVvVoLVqRdFJB8UAAks0KEhEJ39VXX82yZcs477zzMDP+6Z/+ieHDh7N48WIWLVpETk4OxcXF3H///ezevZvrrruOZDIJwD/+4z/2aC0Kkk4qLBkIgCtIRCRE9fX1QOod6IsWLWLRokVveHzhwoUsXLjwTfv19CoknU5tdVJeXgFtHsVbFSQiIukUJJ1kkQgNVkCktT7sUkRETisKki5oooBIm4JERCSdgqQLmiNFxNoawi5DROS0oiDpgpZoITkJBYmISDoFSRe0RgvJTTSGXYaIyGlFQdIF8VgReUkFiYhIOgVJF8RjRRQoSERE3kBB0gXJ3GIKvSnsMkQki23fvp2zzz6bG264gWnTpvGxj32MZ599losvvpiJEyeyfPlyli9fzkUXXcTMmTO56KKL2LhxIwCJRIJbb72VOXPmcO655/KjH/2oR2rSO9u7IreEQppJJhJEotGwqxGRMD15G+xb27PHHD4drvjOW262ZcsWfv3rX3PPPfcwZ84cfvnLX/LSSy/x+OOPc8cdd3D//ffzwgsvEIvFePbZZ/nKV77CI488wr333ktpaSkrVqygpaWFiy++mPe85z2MGzcuo7J7LUjM7CfA+4Ead58WjC0CPgC0An8GrnP32uCx24HrgQTwOXdfEozPAu4DCoAngFvc3c0sD7gfmAUcAj7q7tt7az4A5BUTMae+4SjFAwb16lOJiJzMuHHjmD59OgBTp07lsssuw8yYPn0627dvp66ujoULF7J582bMjLa2NgCefvpp/vSnP/Hwww8DUFdXx+bNm0/fICH1y//fSP2yb/cMcLu7x83su8DtwJfNbAqwAJgKjASeNbNJ7p4A7gZuBF4mFSSXA0+SCp0j7n6WmS0Avgt8tBfng+WnPkq+qb5OQSKS7TqxcugteXl5x29HIpHj9yORCPF4nK997Wu8853v5De/+Q3bt2/n0ksvBVIfPf+DH/yA+fPn92g9vXaNxN1fAA6fMPa0u8eDuy8DlcHtK4EH3b3F3bcBW4ALzGwEMMDdl3nqw/fvB65K22dxcPth4DIzs96aD0A0P9UlsVHtdkXkNFZXV0dFRQUA99133/Hx+fPnc/fddx9foWzatImGhszfGxfmxfZPk1pZAFQAu9Ieqw7GKoLbJ46/YZ8gnOqAso6eyMxuNLMqM6s6cOBAtwuOFaRWJC1qtysip7EvfelL3H777Vx88cUkEonj4zfccANTpkzh/PPPZ9q0afzt3/4t8Xj8FEfqnFAutpvZV4E48Iv2oQ4281OMn2qfNw+63wPcAzB79uyO24p1Qk5hqrlVa6OCRETCMXbs2OO91+GNK470xzZt2nR8/Fvf+haQOvV1xx13cMcdd/RoTX2+IjGzhaQuwn/M/9IrshoYlbZZJbAnGK/sYPwN+5hZDCjlhFNpPS2vKLUiaWs82ptPIyJyRunTIDGzy4EvAx909/R39j0OLDCzPDMbB0wElrv7XuCYmc0Nrn98EngsbZ/27i0fBv7gJ2ti3EPyitrb7SpIRETa9ebLfx8ALgWGmFk18HVSr9LKA54Jrou/7O5/5+7rzOwhYD2pU143B6/YAriJv7z890n+cl3lXuBnZraF1EpkQW/NpZ3a7YqIu9PLr+sJVXf+P95rQeLu13YwfO8ptv828O0OxquAaR2MNwPXZFJjVxUF7XaTzVqRiGSj/Px8Dh06RFlZWb8ME3fn0KFD5Ofnd2k/vbO9C/ILioh7BNQlUSQrVVZWUl1dTSav/jzd5efnU1lZ+dYbplGQdIFFIjRaAZEWndoSyUY5OTkZvwu8P9KHNnZRIwVE1CVRROQ4BUkXNUcKicYVJCIi7RQkXdQSKSQnrmskIiLtFCRdpHa7IiJvpCDporZYsdrtioikUZB0USJH7XZFRNIpSLoomVNEAWq3KyLSTkHSRcncEoq8CU8mwy5FROS0oCDpqrwSouY0NepNiSIioCDpskh7l8R69SQREQEFSZe1t9ttOnYk5EpERE4PCpIuag+S5gZ9ArCICChIuiwnaG7VqiAREQEUJF2WF/Rtb1PfdhERQEHSZe1929VuV0QkRUHSRYXFgwBIqEuiiAigIOmygpLUqS1X33YREUBB0mWFRQNIuuFqtysiAihIuswiERrIx9RuV0QEUJB0S5Op3a6ISDsFSTc0RYqIqUuiiAigIOmWlkgBMfVtFxEBFCTd0hotJDeu5lYiIqAg6Za2WDH5Sa1IRERAQdIt8VgR+Ul1SRQRAQVJtyRziilAp7ZEREBB0i3J3GK12xURCShIuiOvmJglaWnWqkREREHSDZH81CcANxyrDbcQEZHTgIKkGyJ57e12a8MtRETkNNBrQWJmPzGzGjN7LW1ssJk9Y2abg++D0h673cy2mNlGM5ufNj7LzNYGj33fzCwYzzOzXwXjr5jZ2N6ay4liQXOr5gY1txIR6c0VyX3A5SeM3QYsdfeJwNLgPmY2BVgATA32+aGZRYN97gZuBCYGX+3HvB444u5nAXcC3+21mZwgpzB1aqtFQSIi0ntB4u4vAIdPGL4SWBzcXgxclTb+oLu3uPs2YAtwgZmNAAa4+zJ3d+D+E/ZpP9bDwGXtq5Xelqt2uyIix/X1NZJh7r4XIPg+NBivAHalbVcdjFUEt08cf8M+7h4H6oCyXqs8TX5xKkjUbldE5PS52N7RSsJPMX6qfd58cLMbzazKzKoOHDjQzRL/4ni7XQWJiEifB8n+4HQVwfeaYLwaGJW2XSWwJxiv7GD8DfuYWQwo5c2n0gBw93vcfba7zy4vL894En9pt6sgERHp6yB5HFgY3F4IPJY2viB4JdY4UhfVlwenv46Z2dzg+scnT9in/VgfBv4QXEfpdWq3KyLyF7HeOrCZPQBcCgwxs2rg68B3gIfM7HpgJ3ANgLuvM7OHgPVAHLjZ3RPBoW4i9QqwAuDJ4AvgXuBnZraF1EpkQW/N5UQWiXDMCtRuV0SEXgwSd7/2JA9ddpLtvw18u4PxKmBaB+PNBEEUhiYKiLZpRSIicrpcbD/jNEUKiapvu4iIgqS7WiJF5Khvu4iIgqS7WqOF5Ca0IhERUZB0U1usmLykPkZeRERB0k3xnGIKFCQiIgqS7krmFFHoChIREQVJN3leCUU0q92uiGQ9BUk3WW4xEXMaG/QxKSKS3RQk3WRBu91GdUkUkSynIOmmaEEqSJrqa8MtREQkZAqSbooFQdJcr+ZWIpLdFCTdlBN0SWxtqA23EBGRkClIuimvKGi326RPABaR7KYg6ab8IEji6tsuIllOQdJNBSUDAUiqS6KIZDkFSTcVDUj1bU+quZWIZDkFSTfl5RXQ5lFQkIhIllOQdJNFIjRYARH1bReRLKcgyUCjFRJRu10RyXIKkgy0WCExBYmIZDkFSQaao4XkxtUlUUSym4IkA63RInLV3EpEspyCJAPxWBH56tsuIllOQZKBRE4xBeqSKCJZrlNBYma3mNkAS7nXzFaZ2Xt6u7jTXTK3hEJvCrsMEZFQdXZF8ml3Pwq8BygHrgO+02tVnSE8t5hCayERj4ddiohIaDobJBZ8fy/wU3d/NW0sa1l+CQAN6kkiIlmss0Gy0syeJhUkS8ysBEj2XllnhsjxdrtHQq5ERCQ8sU5udz0wA9jq7o1mNpjU6a2sFg1WJM1qtysiWayzK5ILgY3uXmtmHwf+N5D153NiQZfE5oas/6MQkSzW2SC5G2g0s/OALwE7gPt7raozRF4QJG0KEhHJYp0Nkri7O3Al8D13/x5Q0ntlnRly29vtqkuiiGSxzl4jOWZmtwOfAN5hZlEgp/fKOjPkFw8EIN6sniQikr06uyL5KNBC6v0k+4AKYFF3n9TMvmBm68zsNTN7wMzyzWywmT1jZpuD74PStr/dzLaY2UYzm582PsvM1gaPfd/M+vQlyUXt7Xab1G5XRLJXp4IkCI9fAKVm9n6g2d27dY3EzCqAzwGz3X0aEAUWALcBS919IrA0uI+ZTQkenwpcDvwwWBFB6trNjcDE4Ovy7tTUXYVBkLi6JIpIFuvsR6R8BFgOXAN8BHjFzD6cwfPGgAIziwGFwB5S118WB48vBq4Kbl8JPOjuLe6+DdgCXGBmI4AB7r4suH5zf9o+fSInN49mz8EUJCKSxTp7jeSrwBx3rwEws3LgWeDhrj6hu+82s38GdgJNwNPu/rSZDXP3vcE2e81saLBLBfBy2iGqg7G24PaJ429iZjeSWrkwevTorpZ8Sg1WiKm5lYhksc5eI4m0h0jgUBf2fYPg2seVwDhgJFAUvDflpLt0MOanGH/zoPs97j7b3WeXl5d3teRTarICourbLiJZrLMrkqfMbAnwQHD/o8AT3XzOdwPb3P0AgJk9ClwE7DezEcFqZATQHlzVwKi0/StJnQqrDm6fON6nmiNFxNQlUUSyWGcvtt8K3AOcC5wH3OPuX+7mc+4E5ppZYfAqq8uADcDjwMJgm4XAY8Htx4EFZpZnZuNIXVRfHpwGO2Zmc4PjfDJtnz7TEi0kV82tRCSLdXZFgrs/AjyS6RO6+ytm9jCwCogDq0mFVDHwkJldTypsrgm2X2dmDwHrg+1vdvdEcLibgPuAAuDJ4KtPtUWLKGmteesNRUT6qVMGiZkdo+PrDga4uw/ozpO6+9eBr58w3EJqddLR9t8Gvt3BeBUwrTs19JR4TjF5zdvDLEFEJFSnDBJ3z/qPQXkriZxiCtVuV0SymHq2Z8hziihSu10RyWIKkgx5Xgl51kZrS3PYpYiIhEJBkiE73iWxNtxCRERCoiDJUCTokqggEZFspSDJUE7RYAB2v/psyJWIiIRDQZKhsy++ktdj5zBz9ddY+fsfh12OiEifU5BkqKCohMrPPcmm3CnMWP5Fqh6/O+ySRET6lIKkBxQPGMTYW57g9bxzOX/l7ay48yNUb3kt7LJERPqEgqSHFBaXMv6W37N8+AKm1/6B4T97Byv+dQEH9mwPuzQRkV6lIOlBBUUlzL3p36m/aRVVwz/C9CPPknfPRaz47V14Mhl2eSIivUJB0guGDB/N3Jt+xIFPPMfunDHMWfMVXl10hVYnItIvKUh60aizpjPpyy/y8qQvMrlxFbn3XETV7+7R6kRE+hUFSS+LxmLM/ZuvcfDjz7IvNorZVbey+l+upPbgvrBLExHpEQqSPjJq4nmcddsfeXn855h27L9puGseOzauCbssEZGMKUj6UDQWY+4nv8W2D/yafG9m0APv5bUX+7ypo4hIj1KQhGDy7HfR8qlnOBwpY/Kz17Hi0e+FXZKISLcpSEIycuxkBn/uv9hQMIM5f/oHlt37RV2EF5EzkoIkRAMGlnHO/3yS5QPfy4W7/oMV3/8Yba0tYZclItIlCpKQ5eTmMedzv2DZqM9wQe0T/PmfLmH7hqqwyxIR6TQFyWnAIhEuvP6fqZq9iGHxaioefA/L/uMWGuvrwi5NROQtmbuHXUOfmj17tldVnb7/4z9yYC9bfn4Lc+qWkHSjxso4lFvBscHTGHzBR5g4Yx4WUf6LSN8ys5XuPrvDxxQkp6cNryyhdt2z5NRuo6RxF+PaNpNrCfbYUHaMuIIx77mZkWMnh12miGQJBUmaMyVITlR3+AAbn3+Q/I2PMbWpCgP+VDQXn/Fxxs68jEHlI8IuUUT6MQVJmjM1SNLt27mZbUvuYtLuRykjdR1ll41k34BpJIbPZODEtzF26lzyC4pCrlRE+gsFSZr+ECTtWlua2bLqOeo2/zf5+6oY1bieIdQC0Ow5bCh+G8kpV3POJddQWFwabrEickZTkKTpT0FyIk8m2b97K3vWL6Nt83NMOLiUIdTS5LlsKpxB85h3UXnBVVSMPyfsUkXkDKMgSdOfg+REiXic15c/Tf3qR6g4+EcqfS8AOyKj2Dt0HgPO+wCT5/wV0Vgs5EpF5HSnIEmTTUFyouotr1G9/LcU7VjK5OZXybUEBxnIlvK/YuDbrmXy+e/US4tFpEMKkjTZHCTp6o8eYeNLv8XWPcLU+pfJszZ22zB2VryPke9YyJjJM8IuUUROIwqSNAqSNztae4iN//UA+RseYUrzaqLmrCm8kLK/XsSos6aHXZ6InAYUJGkUJKd2cM8ONj99N9O33UcurawasYBzPvJNSgeXh12aiIToVEESyglxMxtoZg+b2etmtsHMLjSzwWb2jJltDr4PStv+djPbYmYbzWx+2vgsM1sbPPZ9M7Mw5tOfDBk5hgs/9R2ab1rOmkHzuWDvL8n53lRevusG9mx7PezyROQ0FNaV1e8BT7n72cB5wAbgNmCpu08Elgb3MbMpwAJgKnA58EMziwbHuRu4EZgYfF3el5Poz4YMH80Fn3+A7dcsYd3AS5lV8yjD7pvLKz9YSMOx2rDLE5HTSJ8HiZkNAOYB9wK4e6u71wJXAouDzRYDVwW3rwQedPcWd98GbAEuMLMRwAB3X+ap83P3p+0jPWT8tLcx5wsPceTGlawY+mHmHHyMI//yNl5f8WzYpYnIaSKMFcl44ADwUzNbbWY/NrMiYJh76o0OwfehwfYVwK60/auDsYrg9onjb2JmN5pZlZlVHThwoGdnkyWGVoxj7s0/5vXLHyBCkom/+zDLfvpldXUUkVCCJAacD9zt7jOBBoLTWCfR0XUPP8X4mwfd73H32e4+u7xcF40zMeXCKyj5/CusLr2MC3f8O1Xf+yitLc1hlyUiIQojSKqBand/Jbj/MKlg2R+criL4XpO2/ai0/SuBPcF4ZQfj0stKSgcz6/O/ZtmYv2NO3dNs/n9/Rd1hrfREslWfB4m77wN2mVl7M43LgPXA48DCYGwh8Fhw+3FggZnlmdk4UhfVlwenv46Z2dzg1VqfTNtHeplFIlx43XepOv+7TGxZz5F/excH9+166x1FpN8J60OWPgv8wsxyga3AdaRC7SEzux7YCVwD4O7rzOwhUmETB25290RwnJuA+4AC4MngS/rQ7A/+Ha8NHsmEZ66n5p4r4MYnGDJ8dNhliUgf0hsSpUes++8nGLfkUxyMDqHghicoHzk27JJEpAeddm9IlP5n6kXvZfsV91OWOET9j6+kpbkx7JJEpI8oSKTHTJl7OVsu+R7jkttZ9bOvhF2OiPQRBYn0qPPetYAVpZczp3oxW159KexyRKQPKEikx01a+G8csVKij92s95iIZAEFifS40sHl7H77PzIuuZ2VP/9q2OWISC9TkEivmPHua6ka8G5m7byP/dV/DrscEelFChLpNRUfugNwtv/m/4Rdioj0IgWJ9JoRYyazesgHmHnwP9m7Y2PY5YhIL1GQSK8ae/U/AMau32pVItJfKUikVw2rnMDqoVdx/uEn2L11XdjliEgvUJBIr5tw9T8QJ8qex78Vdiki0gsUJNLrhowcw5phVzPzyBIO1+wOuxwR6WEKEukTQ+fdQMySbH7u52GXIiI9TEEifWLclDlsj4xiwBa1jBHpbxQk0icsEmHv6PdzTts69u3cHHY5ItKDFCTSZ0bP+wQA25//WciViEhPUpBIn6kYP5VNsUkM2f67sEsRkR6kIJE+dXj8Bzkr8Wd2bFwTdiki0kMUJNKnJlz6CZJu7HlJp7dE+gsFifSp8pFjWZ9/HpXVT+DJZNjliEgPUJBIn2s46wOM8j3s3LQm7FJEpAcoSKTPjZr9fgD2rn4y5EpEpCcoSKTPjRx3NrtsJAU7nw+7FBHpAQoSCcWesrlMbFyjnu4i/YCCREKRO+kyCq2FzSv/EHYpIpIhBYmE4qy3vZe4Rzi6bknYpYhIhhQkEoqS0sFszj2bIfv/GHYpIpIhBYmEpnbEO5jQtoXag/vCLkVEMqAgkdAMOnc+EXP+vPz3YZciIhlQkEhozjrvHRylkMTmpWGXIiIZUJBIaGI5uWwpmsXoI6/o41JEzmAKEglVfNy7GM5BNryiV2+JnKlCCxIzi5rZajP7XXB/sJk9Y2abg++D0ra93cy2mNlGM5ufNj7LzNYGj33fzCyMuUj3Tb/iM9QwmOjSb2hVInKGCnNFcguwIe3+bcBSd58ILA3uY2ZTgAXAVOBy4IdmFg32uRu4EZgYfF3eN6VLTykoKmH79FuYHH+d1UsWh12OiHRDKEFiZpXA+4Afpw1fCbT/JlkMXJU2/qC7t7j7NmALcIGZjQAGuPsyd3fg/rR95Awy68q/Z3tkNEOXf1cfmSJyBgprRfKvwJeA9HMZw9x9L0DwfWgwXgHsStuuOhirCG6fOP4mZnajmVWZWdWBAwd6ZALSc6KxGHVv/xqVvpfVv7kz7HJEpIv6PEjM7P1Ajbuv7OwuHYz5KcbfPOh+j7vPdvfZ5eXlnXxa6UvnXvph1uWex6TXf8jyR+5k99YNb7lPMpFg3R9/T83ubX1QoYicTCyE57wY+KCZvRfIBwaY2c+B/WY2wt33BqetaoLtq4FRaftXAnuC8coOxuUMZJEIBR9cRMvDf8MFa78Ba7/BPsqpyR9DU1ElPnAMAye/gwkz5pGTm8frVUuJPHUbU+ObaPQ8lo37NDM/+jXyC4p6rKaW5kZ2rF9OU+0+zrn4KnLz8nvs2CL9iaUuL4T05GaXAl909/eb2SLgkLt/x8xuAwa7+5fMbCrwS+ACYCSpC/ET3T1hZiuAzwKvAE8AP3D3J071nLNnz/aqqqrem5RkxJNJdm5aw741S4hVv8yApmrKE/sYSD0A9V5Adc5Yzo5v4ACD2Drlf5Cz40XOb3iBPTaMncPejReWES0qo3D4BComzWZQ+YhOP//BfTvZ8tS/U7ZrCWPi28i1BAD7GML2Sdcx/QN/z9HD+6n582pajuxl4rwFXTq+yJnKzFa6++wOHzuNgqQMeAgYDewErnH3w8F2XwU+DcSBz7v7k8H4bOA+oAB4Evisv8WEFCRnptqD+9ha9RRtm//AkNq11Ayfx7kLvklRyUAAXnvpcfKe+yaj4zvIs7Y37HuAQRyJlZO0GAmL4hbF3AEnEcmjJW8wiYIh5NTv5txjL5JjCTbkTKF2yGzyxszCIjHyVtzNlLbXSLoRsb/8iNV7AWtHf4ypH/oKAwaWveF5mxqOsfXVFxlcMYERYyb39h+RSK86bYMkDAqS/s2TSZoaj1F7cB+Hdq6nYeerRA+sJ6/lMBFvI+IJzBOA4Rg53kJJvJZBXkuz5bJh2AcYedlNjJ40403Hfv2Vpzmy5j+JDBrNgDHnEsst4NjSRZxf/wJHKWRX7lk0FFaSKCyn+NBaJjWvPR5qu2wke8ouhGFTyBs4ksKykSSTCVqOHqKt/hA5xYMYefZchgwfjUXefOkymUhw9MgB4vE2EvFWmhuOcXT/NpoP7sQ9yTnvXkhJ6eBe/tOVbKYgSaMgkY54Mom7E4lG33rjE2x59Y8cfu7fKGnYzpC2vZRzhO2R0ewrv5j8iZfQXPNnCnY9z8TGVym0llMe6xCl7MsdTWPBCOLFI7F4CyVH1jGmZTPF1nTS/Y5SyLqKjzDhvZ8nlpNLc8MxWpqOkWhrIZmIAzBszDmUlg07vk8ykeDw/mqKBw3p0WtL0j8pSNIoSKS3xdtaieXkdjh+uGY3dTW7aDi8B4tEyS8po7C0jGOH9nJ020qi+9dS3LCTQW01DPFDJIiyPWc8tQOn4oMnYLFcLBIjkltIwZDRDBw+jvrDe2l47k5m1L/4htNuHdlPGTV5oylqO8KIxB4KrJWkGzVWxqHcChqKR5McOI7coROI5RWTTLThiVYiOXnkFA4kv3gQZRUTKB005PgxN69+gSPP301h0x6ODZlJ0VlvZ8Tk2eTmF5GTm0tefhHRWBiv65GepCBJoyCRM0UiHsc92WEodWTnpjXsWf4bLJZPJL+EaF4RkVgeFo3hiTjNezcQO7ie0sYdNOYMpmnAOGzQWJKNR4jVbaOkYRdD47sZzNG3fK5qG87+4nMobtrD5PhGGj2PPbFKxsa3EbM3ftRNq8fYlnMWR8pmEBk+DZJxkq1N4EkKR57DyLPnMGT46OPbezLZ4em9U/Fkkj3bN7B71RIi+cWcPe8aigcMOv7Y7q3raTp2mFFnzzptVl9HDuxl28oltG3+AxWHXyFuudRM/wwz3/e35OTmvWn7fbu2ULd/JxPOe3unfyZ6koIkjYJE5NSO1h6iZscG4q3NRGO5RGK5JNqaaak/QltDLS01m8mrWcuIhg20RPLZN/FaplzxdwwYWEZjfR1b1zxPw57X8XgrJFqhvobSw39iXOsm8k94IUS7OoowIN9byLU4DZ7PMSuhIVrCsbxhNBdVwsBReFsTkfr95DQdxDx+fP9hTVsY6TXH77d4DuuL5hDPGcCouiqGcxCANo+yMzaGY3nDyGurozBxlPxkE0kiJCxKm+VTW1BJc+l4ooPHES0sJZZfQm5RKaXDxlI+ciw5uXm0tbZQU72V2r1bibc24vE2Em1NtO7fRN6h9ZQ1buNoTjn1I95G6dmXErEIx/ZuJn5oK/mHNjCicRPDSb05ut4L2FI0g5KW/UxIbGUf5WwbMR8iOYATa9hPRd0qRvp+AI4wgM2D5hE9650k460kGmvx5jpItGLx1KnTSPkkBk2YxeizZ5GXX9gjPxcKkjQKEpFwtLY0U7NrC7G8fPILikkmE+zdvIZjO1ZjhzbjkRw8VgCxXKy1nmhzLTmttZS27GNoYh9Flvr4nKMUcSQyiLjlYp7EcGrzK2gdcwkjZl5O45Eaalf+mvE1z5JDG9uKZ9I2eh45JUNo2bWaosOvUdx6iKZYKS25pcRjxanjeJxYWz2DWqoZkdh7/KXf6RJu1FkJpX6M6ElOI1bbcA4UjGdg8x7GJbe/4bGkG7sjI6gpPpu2YecycOLFTJh5CTm5eXgyyZ/+62Fyl93JpNbUG3Id45gVsb3wXFoqLyJWOgxef5Jzjv7x+J9H+rFbiWFw/EUeCTcarIBm8mmOFFAz6wvMft9nuvX3pyBJoyAROfN4Mknd4RryC4vJLyzu0n5dPU0GqetZB/fuoKXhKM0NdbTUH6H50E4SR3YSbaghUTSU2OCxFJSPJVZQTDSWSzSWy9DRk46fUoPUy9a3r3kOi0QZVDmJYaMn9cgKobmpgepNa8grGkBx6RCKBgwiJycXi0RIJhLs2baB/ZtW0Lr3Nay5jkhbA9FEE7lzFjJ93tXdek4FSRoFiYhI150qSNTYSkREMqIgERGRjChIREQkIwoSERHJiIJEREQyoiAREZGMKEhERCQjChIREclI1r0h0cwOADu6ufsQCD60J7tk47yzcc6QnfPOxjlD1+c9xt3LO3og64IkE2ZWdbJ3dvZn2TjvbJwzZOe8s3HO0LPz1qktERHJiIJEREQyoiDpmnvCLiAk2TjvbJwzZOe8s3HO0IPz1jUSERHJiFYkIiKSEQWJiIhkREHSSWZ2uZltNLMtZnZb2PX0BjMbZWbPmdkGM1tnZrcE44PN7Bkz2xx8H/RWxzrTmFnUzFab2e+C+9kw54Fm9rCZvR78nV/Y3+dtZl8IfrZfM7MHzCy/P87ZzH5iZjVm9lra2EnnaWa3B7/bNprZ/K4+n4KkE8wsCtwFXAFMAa41synhVtUr4sD/cvdzgLnAzcE8bwOWuvtEYGlwv7+5BdiQdj8b5vw94Cl3Pxs4j9T8++28zawC+Bww292nAVFgAf1zzvcBl58w1uE8g3/jC4CpwT4/DH7ndZqCpHMuALa4+1Z3bwUeBK4MuaYe5+573X1VcPsYqV8sFaTmujjYbDFwVSgF9hIzqwTeB/w4bbi/z3kAMA+4F8DdW929ln4+byAGFJhZDCgE9tAP5+zuLwCHTxg+2TyvBB509xZ33wZsIfU7r9MUJJ1TAexKu18djPVbZjYWmAm8Agxz972QChtgaIil9YZ/Bb4EJNPG+vucxwMHgJ8Gp/R+bGZF9ON5u/tu4J+BncBeoM7dn6Yfz/kEJ5tnxr/fFCSdYx2M9dvXTZtZMfAI8Hl3Pxp2Pb3JzN4P1Lj7yrBr6WMx4HzgbnefCTTQP07pnFRwTeBKYBwwEigys4+HW9VpIePfbwqSzqkGRqXdryS1JO53zCyHVIj8wt0fDYb3m9mI4PERQE1Y9fWCi4EPmtl2Uqcs32VmP6d/zxlSP9PV7v5KcP9hUsHSn+f9bmCbux9w9zbgUeAi+vec051snhn/flOQdM4KYKKZjTOzXFIXph4PuaYeZ2ZG6pz5Bnf/l7SHHgcWBrcXAo/1dW29xd1vd/dKdx9L6u/1D+7+cfrxnAHcfR+wy8wmB0OXAevp3/PeCcw1s8LgZ/0yUtcB+/Oc051sno8DC8wsz8zGAROB5V05sN7Z3klm9l5S59KjwE/c/dvhVtTzzOztwIvAWv5yveArpK6TPASMJvWP8Rp3P/FC3hnPzC4Fvuju7zezMvr5nM1sBqkXGOQCW4HrSP3nst/O28y+CXyU1CsUVwM3AMX0szmb2QPApaQ+Kn4/8HXgt5xknmb2VeDTpP5cPu/uT3bp+RQkIiKSCZ3aEhGRjChIREQkIwoSERHJiIJEREQyoiAREZGMKEhETnNmdmn7pxKLnI4UJCIikhEFiUgPMbOPm9lyM1tjZj8KepzUm9n/M7NVZrbUzMqDbWeY2ctm9icz+017bwgzO8vMnjWzV4N9JgSHL07rHfKL4J3ZmNl3zGx9cJx/DmnqkuUUJCI9wMzOIfWO6YvdfQaQAD4GFAGr3P184HlS7zAGuB/4srufS+qTBNrHfwHc5e7nkfocqL3B+Ezg86T64YwHLjazwcDVwNTgOP+3N+cocjIKEpGecRkwC1hhZmuC++NJfdTMr4Jtfg683cxKgYHu/nwwvhiYZ2YlQIW7/wbA3ZvdvTHYZrm7V7t7ElgDjAWOAs3Aj83sr4H2bUX6lIJEpGcYsNjdZwRfk939Gx1sd6rPJOro47zbtaTdTgAxd4+TakD0CKkmRU91rWSRnqEgEekZS4EPm9lQON4fewypf2MfDrb5G+Ald68DjpjZO4LxTwDPB71fqs3squAYeWZWeLInDPrGlLr7E6ROe83o8VmJdEIs7AJE+gN3X29m/xt42swiQBtwM6mGUVPNbCVQR+o6CqQ+xvvfg6Bo/+RdSIXKj8zs/wTHuOYUT1sCPGZm+aRWM1/o4WmJdIo+/VekF5lZvbsXh12HSG/SqS0REcmIViQiIpIRrUhERCQjChIREcmIgkRERDKiIBERkYwoSEREJCP/H3rZxodghDr2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot history (also known as a loss curve)\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8de61",
   "metadata": {},
   "source": [
    "From this, it looks like our model's loss (and MAE) were both still decreasing (in our case, MAE and loss are the same, hence the lines in the plot overlap eachother).\n",
    "\n",
    "What this tells us is the loss might go down if we try training it for longer.\n",
    "\n",
    "> ü§î **Question:** How long should you train for? \n",
    "\n",
    "> It depends on what problem you're working on. Sometimes training won't take very long, other times it'll take longer than you expect. A common method is to set your model training for a very long time (e.g. 1000's of epochs) but set it up with an [EarlyStopping callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) so it stops automatically when it stops improving. We'll see this in another module.\n",
    "\n",
    "Let's train the same model as above for a little longer. We can do this but calling fit on it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59936405",
   "metadata": {},
   "source": [
    "## Preprocessing data (normalization and standardization)\n",
    "\n",
    "A common practice when working with neural networks is to make sure all of the data you pass to them is in the range 0 to 1.\n",
    "\n",
    "This practice is called **normalization** (scaling all values from their original range to, e.g. between 0 and 100,000 to be between 0 and 1).\n",
    "\n",
    "There is another process call **standardization** which converts all of your data to unit variance and 0 mean.\n",
    "\n",
    "These two practices are often part of a preprocessing pipeline (a series of functions to prepare your data for use with neural networks).\n",
    "\n",
    "Knowing this, some of the major steps you'll take to preprocess your data for a neural network include:\n",
    "* Turning all of your data to numbers (a neural network can't handle strings).\n",
    "* Making sure your data is in the right shape (verifying input and output shapes).\n",
    "* [**Feature scaling**](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler):\n",
    "    * Normalizing data (making sure all values are between 0 and 1). This is done by subtracting the minimum value then dividing by the maximum value minus the minmum. This is also referred to as min-max scaling.\n",
    "    * Standardization (making sure all values have a mean of 0 and a variance of 1). This is done by substracting the mean value from the target feature and then dividing it by the standard deviation.\n",
    "    * Which one should you use?\n",
    "      * **With neural networks you'll tend to favour normalization** as they tend to prefer values between 0 and 1 (you'll see this espcially with image processing), however, you'll often find a neural network can perform pretty well with minimal feature scaling.\n",
    "\n",
    "> üìñ **Resource:** For more on preprocessing data, I'd recommend reading the following resources:\n",
    "* [Scikit-Learn's documentation on preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-data).\n",
    "* [Scale, Standardize or Normalize with Scikit-Learn by Jeff Hale](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02).\n",
    "\n",
    "We've already turned our data into numbers using `get_dummies()`, let's see how we'd normalize it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "88020bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture - 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74cfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
